[
  {
    "arxiv": {
      "arxiv_id": "2406.09246",
      "title": "OpenVLA: An Open-Source Vision-Language-Action Model",
      "authors": [
        "Moo Jin Kim",
        "Karl Pertsch",
        "Siddharth Karamcheti",
        "Ted Xiao",
        "Ashwin Balakrishna",
        "Suraj Nair",
        "Rafael Rafailov",
        "Ethan Foster",
        "Grace Lam",
        "Pannag Sanketi",
        "Quan Vuong",
        "Thomas Kollar",
        "Benjamin Burchfiel",
        "Russ Tedrake",
        "Dorsa Sadigh",
        "Sergey Levine",
        "Percy Liang",
        "Chelsea Finn"
      ],
      "summary": "Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.",
      "published_date": "2024-06-13",
      "pdf_url": "https://arxiv.org/pdf/2406.09246.pdf",
      "arxiv_url": "https://arxiv.org/abs/2406.09246",
      "categories": [
        "cs.RO",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": "https://github.com/openvla/openvla",
      "stars": 4415,
      "forks": 527,
      "last_commit": "2025-03-23T23:41:01Z",
      "open_prs": 7,
      "open_issues": 91,
      "watchers": 4415,
      "latest_pr_date": "2025-11-09T10:59:38Z"
    },
    "twitter": {
      "tweet_url": "https://x.com/moo_jin_kim/status/1801548441102991771",
      "tweet_id": "1801548441102991771",
      "likes": 695,
      "retweets": 162,
      "replies": 32,
      "quotes": 27,
      "views": 226126
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "vla"
    ],
    "tags": [
      "VLA",
      "vision-language-action",
      "open-source",
      "Stanford"
    ],
    "added_date": "2025-11-13T23:41:11.898618"
  },
  {
    "arxiv": {
      "arxiv_id": "2307.15818",
      "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
      "authors": [
        "Anthony Brohan",
        "Noah Brown",
        "Justice Carbajal",
        "Yevgen Chebotar",
        "Xi Chen",
        "Krzysztof Choromanski",
        "Tianli Ding",
        "Danny Driess",
        "Avinava Dubey",
        "Chelsea Finn",
        "Pete Florence",
        "Chuyuan Fu",
        "Montse Gonzalez Arenas",
        "Keerthana Gopalakrishnan",
        "Kehang Han",
        "Karol Hausman",
        "Alexander Herzog",
        "Jasmine Hsu",
        "Brian Ichter",
        "Alex Irpan",
        "Nikhil Joshi",
        "Ryan Julian",
        "Dmitry Kalashnikov",
        "Yuheng Kuang",
        "Isabel Leal",
        "Lisa Lee",
        "Tsang-Wei Edward Lee",
        "Sergey Levine",
        "Yao Lu",
        "Henryk Michalewski",
        "Igor Mordatch",
        "Karl Pertsch",
        "Kanishka Rao",
        "Krista Reymann",
        "Michael Ryoo",
        "Grecia Salazar",
        "Pannag Sanketi",
        "Pierre Sermanet",
        "Jaspiar Singh",
        "Anikait Singh",
        "Radu Soricut",
        "Huong Tran",
        "Vincent Vanhoucke",
        "Quan Vuong",
        "Ayzaan Wahid",
        "Stefan Welker",
        "Paul Wohlhart",
        "Jialin Wu",
        "Fei Xia",
        "Ted Xiao",
        "Peng Xu",
        "Sichun Xu",
        "Tianhe Yu",
        "Brianna Zitkovich"
      ],
      "summary": "We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).",
      "published_date": "2023-07-28",
      "pdf_url": "https://arxiv.org/pdf/2307.15818.pdf",
      "arxiv_url": "https://arxiv.org/abs/2307.15818",
      "categories": [
        "cs.RO",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": "https://github.com/kyegomez/RT-2",
      "stars": 528,
      "forks": 66,
      "last_commit": "2024-07-26T17:08:46Z",
      "open_prs": 3,
      "open_issues": 12,
      "watchers": 528,
      "latest_pr_date": "2024-12-31T10:25:25Z"
    },
    "twitter": {
      "tweet_url": "https://x.com/GoogleDeepMind/status/1684903412834447360",
      "tweet_id": "1684903412834447360",
      "likes": 1590,
      "retweets": 435,
      "replies": 38,
      "quotes": 78,
      "views": 537063
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "vla"
    ],
    "tags": [
      "VLA",
      "vision-language-action",
      "Google",
      "RT-2"
    ],
    "added_date": "2025-11-13T23:41:11.898647"
  },
  {
    "arxiv": {
      "arxiv_id": "2310.08864",
      "title": "Open X-Embodiment: Robotic Learning Datasets and RT-X Models",
      "authors": [
        "Open X-Embodiment Collaboration",
        "Abby O'Neill",
        "Abdul Rehman",
        "Abhinav Gupta",
        "Abhiram Maddukuri",
        "Abhishek Gupta",
        "Abhishek Padalkar",
        "Abraham Lee",
        "Acorn Pooley",
        "Agrim Gupta",
        "Ajay Mandlekar",
        "Ajinkya Jain",
        "Albert Tung",
        "Alex Bewley",
        "Alex Herzog",
        "Alex Irpan",
        "Alexander Khazatsky",
        "Anant Rai",
        "Anchit Gupta",
        "Andrew Wang",
        "Andrey Kolobov",
        "Anikait Singh",
        "Animesh Garg",
        "Aniruddha Kembhavi",
        "Annie Xie",
        "Anthony Brohan",
        "Antonin Raffin",
        "Archit Sharma",
        "Arefeh Yavary",
        "Arhan Jain",
        "Ashwin Balakrishna",
        "Ayzaan Wahid",
        "Ben Burgess-Limerick",
        "Beomjoon Kim",
        "Bernhard Sch\u00f6lkopf",
        "Blake Wulfe",
        "Brian Ichter",
        "Cewu Lu",
        "Charles Xu",
        "Charlotte Le",
        "Chelsea Finn",
        "Chen Wang",
        "Chenfeng Xu",
        "Cheng Chi",
        "Chenguang Huang",
        "Christine Chan",
        "Christopher Agia",
        "Chuer Pan",
        "Chuyuan Fu",
        "Coline Devin",
        "Danfei Xu",
        "Daniel Morton",
        "Danny Driess",
        "Daphne Chen",
        "Deepak Pathak",
        "Dhruv Shah",
        "Dieter B\u00fcchler",
        "Dinesh Jayaraman",
        "Dmitry Kalashnikov",
        "Dorsa Sadigh",
        "Edward Johns",
        "Ethan Foster",
        "Fangchen Liu",
        "Federico Ceola",
        "Fei Xia",
        "Feiyu Zhao",
        "Felipe Vieira Frujeri",
        "Freek Stulp",
        "Gaoyue Zhou",
        "Gaurav S. Sukhatme",
        "Gautam Salhotra",
        "Ge Yan",
        "Gilbert Feng",
        "Giulio Schiavi",
        "Glen Berseth",
        "Gregory Kahn",
        "Guangwen Yang",
        "Guanzhi Wang",
        "Hao Su",
        "Hao-Shu Fang",
        "Haochen Shi",
        "Henghui Bao",
        "Heni Ben Amor",
        "Henrik I Christensen",
        "Hiroki Furuta",
        "Homanga Bharadhwaj",
        "Homer Walke",
        "Hongjie Fang",
        "Huy Ha",
        "Igor Mordatch",
        "Ilija Radosavovic",
        "Isabel Leal",
        "Jacky Liang",
        "Jad Abou-Chakra",
        "Jaehyung Kim",
        "Jaimyn Drake",
        "Jan Peters",
        "Jan Schneider",
        "Jasmine Hsu",
        "Jay Vakil",
        "Jeannette Bohg",
        "Jeffrey Bingham",
        "Jeffrey Wu",
        "Jensen Gao",
        "Jiaheng Hu",
        "Jiajun Wu",
        "Jialin Wu",
        "Jiankai Sun",
        "Jianlan Luo",
        "Jiayuan Gu",
        "Jie Tan",
        "Jihoon Oh",
        "Jimmy Wu",
        "Jingpei Lu",
        "Jingyun Yang",
        "Jitendra Malik",
        "Jo\u00e3o Silv\u00e9rio",
        "Joey Hejna",
        "Jonathan Booher",
        "Jonathan Tompson",
        "Jonathan Yang",
        "Jordi Salvador",
        "Joseph J. Lim",
        "Junhyek Han",
        "Kaiyuan Wang",
        "Kanishka Rao",
        "Karl Pertsch",
        "Karol Hausman",
        "Keegan Go",
        "Keerthana Gopalakrishnan",
        "Ken Goldberg",
        "Kendra Byrne",
        "Kenneth Oslund",
        "Kento Kawaharazuka",
        "Kevin Black",
        "Kevin Lin",
        "Kevin Zhang",
        "Kiana Ehsani",
        "Kiran Lekkala",
        "Kirsty Ellis",
        "Krishan Rana",
        "Krishnan Srinivasan",
        "Kuan Fang",
        "Kunal Pratap Singh",
        "Kuo-Hao Zeng",
        "Kyle Hatch",
        "Kyle Hsu",
        "Laurent Itti",
        "Lawrence Yunliang Chen",
        "Lerrel Pinto",
        "Li Fei-Fei",
        "Liam Tan",
        "Linxi \"Jim\" Fan",
        "Lionel Ott",
        "Lisa Lee",
        "Luca Weihs",
        "Magnum Chen",
        "Marion Lepert",
        "Marius Memmel",
        "Masayoshi Tomizuka",
        "Masha Itkina",
        "Mateo Guaman Castro",
        "Max Spero",
        "Maximilian Du",
        "Michael Ahn",
        "Michael C. Yip",
        "Mingtong Zhang",
        "Mingyu Ding",
        "Minho Heo",
        "Mohan Kumar Srirama",
        "Mohit Sharma",
        "Moo Jin Kim",
        "Muhammad Zubair Irshad",
        "Naoaki Kanazawa",
        "Nicklas Hansen",
        "Nicolas Heess",
        "Nikhil J Joshi",
        "Niko Suenderhauf",
        "Ning Liu",
        "Norman Di Palo",
        "Nur Muhammad Mahi Shafiullah",
        "Oier Mees",
        "Oliver Kroemer",
        "Osbert Bastani",
        "Pannag R Sanketi",
        "Patrick \"Tree\" Miller",
        "Patrick Yin",
        "Paul Wohlhart",
        "Peng Xu",
        "Peter David Fagan",
        "Peter Mitrano",
        "Pierre Sermanet",
        "Pieter Abbeel",
        "Priya Sundaresan",
        "Qiuyu Chen",
        "Quan Vuong",
        "Rafael Rafailov",
        "Ran Tian",
        "Ria Doshi",
        "Roberto Mart\u00edn-Mart\u00edn",
        "Rohan Baijal",
        "Rosario Scalise",
        "Rose Hendrix",
        "Roy Lin",
        "Runjia Qian",
        "Ruohan Zhang",
        "Russell Mendonca",
        "Rutav Shah",
        "Ryan Hoque",
        "Ryan Julian",
        "Samuel Bustamante",
        "Sean Kirmani",
        "Sergey Levine",
        "Shan Lin",
        "Sherry Moore",
        "Shikhar Bahl",
        "Shivin Dass",
        "Shubham Sonawani",
        "Shubham Tulsiani",
        "Shuran Song",
        "Sichun Xu",
        "Siddhant Haldar",
        "Siddharth Karamcheti",
        "Simeon Adebola",
        "Simon Guist",
        "Soroush Nasiriany",
        "Stefan Schaal",
        "Stefan Welker",
        "Stephen Tian",
        "Subramanian Ramamoorthy",
        "Sudeep Dasari",
        "Suneel Belkhale",
        "Sungjae Park",
        "Suraj Nair",
        "Suvir Mirchandani",
        "Takayuki Osa",
        "Tanmay Gupta",
        "Tatsuya Harada",
        "Tatsuya Matsushima",
        "Ted Xiao",
        "Thomas Kollar",
        "Tianhe Yu",
        "Tianli Ding",
        "Todor Davchev",
        "Tony Z. Zhao",
        "Travis Armstrong",
        "Trevor Darrell",
        "Trinity Chung",
        "Vidhi Jain",
        "Vikash Kumar",
        "Vincent Vanhoucke",
        "Vitor Guizilini",
        "Wei Zhan",
        "Wenxuan Zhou",
        "Wolfram Burgard",
        "Xi Chen",
        "Xiangyu Chen",
        "Xiaolong Wang",
        "Xinghao Zhu",
        "Xinyang Geng",
        "Xiyuan Liu",
        "Xu Liangwei",
        "Xuanlin Li",
        "Yansong Pang",
        "Yao Lu",
        "Yecheng Jason Ma",
        "Yejin Kim",
        "Yevgen Chebotar",
        "Yifan Zhou",
        "Yifeng Zhu",
        "Yilin Wu",
        "Ying Xu",
        "Yixuan Wang",
        "Yonatan Bisk",
        "Yongqiang Dou",
        "Yoonyoung Cho",
        "Youngwoon Lee",
        "Yuchen Cui",
        "Yue Cao",
        "Yueh-Hua Wu",
        "Yujin Tang",
        "Yuke Zhu",
        "Yunchu Zhang",
        "Yunfan Jiang",
        "Yunshuang Li",
        "Yunzhu Li",
        "Yusuke Iwasawa",
        "Yutaka Matsuo",
        "Zehan Ma",
        "Zhuo Xu",
        "Zichen Jeff Cui",
        "Zichen Zhang",
        "Zipeng Fu",
        "Zipeng Lin"
      ],
      "summary": "Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train generalist X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. More details can be found on the project website https://robotics-transformer-x.github.io.",
      "published_date": "2023-10-13",
      "pdf_url": "https://arxiv.org/pdf/2310.08864.pdf",
      "arxiv_url": "https://arxiv.org/abs/2310.08864",
      "categories": [
        "cs.RO"
      ]
    },
    "github": {
      "repo_url": null,
      "stars": null,
      "forks": null,
      "last_commit": null,
      "open_prs": null,
      "open_issues": null,
      "watchers": null,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": "https://x.com/GoogleDeepMind/status/1709207886943965648",
      "tweet_id": "1709207886943965648",
      "likes": 1173,
      "retweets": 302,
      "replies": 25,
      "quotes": 54,
      "views": 518773
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "datasets",
      "vla"
    ],
    "tags": [
      "dataset",
      "open-x-embodiment",
      "benchmark",
      "Google"
    ],
    "added_date": "2025-11-13T23:41:11.898655"
  },
  {
    "arxiv": {
      "arxiv_id": "2210.03094",
      "title": "VIMA: General Robot Manipulation with Multimodal Prompts",
      "authors": [
        "Yunfan Jiang",
        "Agrim Gupta",
        "Zichen Zhang",
        "Guanzhi Wang",
        "Yongqiang Dou",
        "Yanjun Chen",
        "Li Fei-Fei",
        "Anima Anandkumar",
        "Yuke Zhu",
        "Linxi Fan"
      ],
      "summary": "Prompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. We show that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts, interleaving textual and visual tokens. Accordingly, we develop a new simulation benchmark that consists of thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and a four-level evaluation protocol for systematic generalization. We design a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. VIMA features a recipe that achieves strong model scalability and data efficiency. It outperforms alternative designs in the hardest zero-shot generalization setting by up to $2.9\\times$ task success rate given the same training data. With $10\\times$ less training data, VIMA still performs $2.7\\times$ better than the best competing variant. Code and video demos are available at https://vimalabs.github.io/",
      "published_date": "2022-10-06",
      "pdf_url": "https://arxiv.org/pdf/2210.03094.pdf",
      "arxiv_url": "https://arxiv.org/abs/2210.03094",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": "https://github.com/vimalabs/VIMA",
      "stars": 835,
      "forks": 96,
      "last_commit": "2024-04-18T18:57:22Z",
      "open_prs": 2,
      "open_issues": 14,
      "watchers": 835,
      "latest_pr_date": "2024-04-18T19:03:18Z"
    },
    "twitter": {
      "tweet_url": "https://x.com/DrJimFan/status/1578433493561769984",
      "tweet_id": "1578433493561769984",
      "likes": 856,
      "retweets": 147,
      "replies": 18,
      "quotes": 36,
      "views": 0
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "vla"
    ],
    "tags": [
      "VLA",
      "multimodal",
      "prompts",
      "Stanford"
    ],
    "added_date": "2025-11-13T23:41:11.898662"
  },
  {
    "arxiv": {
      "arxiv_id": "2311.12871",
      "title": "An Embodied Generalist Agent in 3D World",
      "authors": [
        "Jiangyong Huang",
        "Silong Yong",
        "Xiaojian Ma",
        "Xiongkun Linghu",
        "Puhao Li",
        "Yan Wang",
        "Qing Li",
        "Song-Chun Zhu",
        "Baoxiong Jia",
        "Siyuan Huang"
      ],
      "summary": "Leveraging massive knowledge from large language models (LLMs), recent machine learning models show notable successes in general-purpose task solving in diverse domains such as computer vision and robotics. However, several significant challenges remain: (i) most of these models rely on 2D images yet exhibit a limited capacity for 3D input; (ii) these models rarely explore the tasks inherently defined in 3D world, e.g., 3D grounding, embodied reasoning and acting. We argue these limitations significantly hinder current models from performing real-world tasks and approaching general intelligence. To this end, we introduce LEO, an embodied multi-modal generalist agent that excels in perceiving, grounding, reasoning, planning, and acting in the 3D world. LEO is trained with a unified task interface, model architecture, and objective in two stages: (i) 3D vision-language (VL) alignment and (ii) 3D vision-language-action (VLA) instruction tuning. We collect large-scale datasets comprising diverse object-level and scene-level tasks, which require considerable understanding of and interaction with the 3D world. Moreover, we meticulously design an LLM-assisted pipeline to produce high-quality 3D VL data. Through extensive experiments, we demonstrate LEO's remarkable proficiency across a wide spectrum of tasks, including 3D captioning, question answering, embodied reasoning, navigation and manipulation. Our ablative studies and scaling analyses further provide valuable insights for developing future embodied generalist agents. Code and data are available on project page.",
      "published_date": "2023-11-18",
      "pdf_url": "https://arxiv.org/pdf/2311.12871.pdf",
      "arxiv_url": "https://arxiv.org/abs/2311.12871",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": "https://github.com/embodied-generalist/embodied-generalist",
      "stars": 465,
      "forks": 40,
      "last_commit": "2025-04-20T11:14:46Z",
      "open_prs": 0,
      "open_issues": 0,
      "watchers": 465,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": "https://x.com/jeasinema/status/1727595460867862930",
      "tweet_id": "1727595460867862930",
      "likes": 199,
      "retweets": 34,
      "replies": 4,
      "quotes": 7,
      "views": 81872
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "vla"
    ],
    "tags": [
      "VLA",
      "3D",
      "simulation",
      "embodied-agent"
    ],
    "added_date": "2025-11-13T23:41:11.898668"
  },
  {
    "arxiv": {
      "arxiv_id": "2503.14734",
      "title": "GR00T N1: An Open Foundation Model for Generalist Humanoid Robots",
      "authors": [
        " NVIDIA",
        " :",
        "Johan Bjorck",
        "Fernando Casta\u00f1eda",
        "Nikita Cherniadev",
        "Xingye Da",
        "Runyu Ding",
        "Linxi \"Jim\" Fan",
        "Yu Fang",
        "Dieter Fox",
        "Fengyuan Hu",
        "Spencer Huang",
        "Joel Jang",
        "Zhenyu Jiang",
        "Jan Kautz",
        "Kaushil Kundalia",
        "Lawrence Lao",
        "Zhiqi Li",
        "Zongyu Lin",
        "Kevin Lin",
        "Guilin Liu",
        "Edith Llontop",
        "Loic Magne",
        "Ajay Mandlekar",
        "Avnish Narayan",
        "Soroush Nasiriany",
        "Scott Reed",
        "You Liang Tan",
        "Guanzhi Wang",
        "Zu Wang",
        "Jing Wang",
        "Qi Wang",
        "Jiannan Xiang",
        "Yuqi Xie",
        "Yinzhen Xu",
        "Zhenjia Xu",
        "Seonghyeon Ye",
        "Zhiding Yu",
        "Ao Zhang",
        "Hao Zhang",
        "Yizhou Zhao",
        "Ruijie Zheng",
        "Yuke Zhu"
      ],
      "summary": "General-purpose robots need a versatile body and an intelligent mind. Recent advancements in humanoid robots have shown great promise as a hardware platform for building generalist autonomy in the human world. A robot foundation model, trained on massive and diverse data sources, is essential for enabling the robots to reason about novel situations, robustly handle real-world variability, and rapidly learn new tasks. To this end, we introduce GR00T N1, an open foundation model for humanoid robots. GR00T N1 is a Vision-Language-Action (VLA) model with a dual-system architecture. The vision-language module (System 2) interprets the environment through vision and language instructions. The subsequent diffusion transformer module (System 1) generates fluid motor actions in real time. Both modules are tightly coupled and jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixture of real-robot trajectories, human videos, and synthetically generated datasets. We show that our generalist robot model GR00T N1 outperforms the state-of-the-art imitation learning baselines on standard simulation benchmarks across multiple robot embodiments. Furthermore, we deploy our model on the Fourier GR-1 humanoid robot for language-conditioned bimanual manipulation tasks, achieving strong performance with high data efficiency.",
      "published_date": "2025-03-18",
      "pdf_url": "https://arxiv.org/pdf/2503.14734.pdf",
      "arxiv_url": "https://arxiv.org/abs/2503.14734",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": null,
      "stars": null,
      "forks": null,
      "last_commit": null,
      "open_prs": null,
      "open_issues": null,
      "watchers": null,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": "https://x.com/DrJimFan/status/1902117478496616642",
      "tweet_id": "1902117478496616642",
      "likes": 1945,
      "retweets": 398,
      "replies": 93,
      "quotes": 121,
      "views": 450145
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "vla"
    ],
    "tags": [
      "VLA",
      "humanoid",
      "NVIDIA",
      "foundation-model"
    ],
    "added_date": "2025-11-13T23:41:11.898675"
  },
  {
    "arxiv": {
      "arxiv_id": "2410.00371",
      "title": "AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation",
      "authors": [
        "Jiafei Duan",
        "Wilbert Pumacay",
        "Nishanth Kumar",
        "Yi Ru Wang",
        "Shulin Tian",
        "Wentao Yuan",
        "Ranjay Krishna",
        "Dieter Fox",
        "Ajay Mandlekar",
        "Yijie Guo"
      ],
      "summary": "Robotic manipulation in open-world settings requires not only task execution but also the ability to detect and learn from failures. While recent advances in vision-language models (VLMs) and large language models (LLMs) have improved robots' spatial reasoning and problem-solving abilities, they still struggle with failure recognition, limiting their real-world applicability. We introduce AHA, an open-source VLM designed to detect and reason about failures in robotic manipulation using natural language. By framing failure detection as a free-form reasoning task, AHA identifies failures and provides detailed, adaptable explanations across different robots, tasks, and environments. We fine-tuned AHA using FailGen, a scalable framework that generates the first large-scale dataset of robotic failure trajectories, the AHA dataset. FailGen achieves this by procedurally perturbing successful demonstrations from simulation. Despite being trained solely on the AHA dataset, AHA generalizes effectively to real-world failure datasets, robotic systems, and unseen tasks. It surpasses the second-best model (GPT-4o in-context learning) by 10.3% and exceeds the average performance of six compared models including five state-of-the-art VLMs by 35.3% across multiple metrics and datasets. We integrate AHA into three manipulation frameworks that utilize LLMs/VLMs for reinforcement learning, task and motion planning, and zero-shot trajectory generation. AHA's failure feedback enhances these policies' performances by refining dense reward functions, optimizing task planning, and improving sub-task verification, boosting task success rates by an average of 21.4% across all three tasks compared to GPT-4 models.",
      "published_date": "2024-10-01",
      "pdf_url": "https://arxiv.org/pdf/2410.00371.pdf",
      "arxiv_url": "https://arxiv.org/abs/2410.00371",
      "categories": [
        "cs.RO"
      ]
    },
    "github": {
      "repo_url": "https://github.com/NVlabs/AHA",
      "stars": 49,
      "forks": 3,
      "last_commit": "2025-04-01T06:55:04Z",
      "open_prs": 0,
      "open_issues": 3,
      "watchers": 49,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": "https://x.com/DJiafei/status/1838562171460161619",
      "tweet_id": "1838562171460161619",
      "likes": 201,
      "retweets": 43,
      "replies": 6,
      "quotes": 9,
      "views": 48462
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "vla"
    ],
    "tags": [
      "VLA",
      "failure-detection",
      "reasoning"
    ],
    "added_date": "2025-11-13T23:41:11.898682"
  },
  {
    "arxiv": {
      "arxiv_id": "2411.19650",
      "title": "CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation",
      "authors": [
        "Qixiu Li",
        "Yaobo Liang",
        "Zeyu Wang",
        "Lin Luo",
        "Xi Chen",
        "Mozheng Liao",
        "Fangyun Wei",
        "Yu Deng",
        "Sicheng Xu",
        "Yizhong Zhang",
        "Xiaofan Wang",
        "Bei Liu",
        "Jianlong Fu",
        "Jianmin Bao",
        "Dong Chen",
        "Yuanchun Shi",
        "Jiaolong Yang",
        "Baining Guo"
      ],
      "summary": "The advancement of large Vision-Language-Action (VLA) models has significantly improved robotic manipulation in terms of language-guided task execution and generalization to unseen scenarios. While existing VLAs adapted from pretrained large Vision-Language-Models (VLM) have demonstrated promising generalizability, their task performance is still unsatisfactory as indicated by the low tasks success rates in different environments. In this paper, we present a new advanced VLA architecture derived from VLM. Unlike previous works that directly repurpose VLM for action prediction by simple action quantization, we propose a omponentized VLA architecture that has a specialized action module conditioned on VLM output. We systematically study the design of the action module and demonstrates the strong performance enhancement with diffusion action transformers for action sequence modeling, as well as their favorable scaling behaviors. We also conduct comprehensive experiments and ablation studies to evaluate the efficacy of our models with varied designs. The evaluation on 5 robot embodiments in simulation and real work shows that our model not only significantly surpasses existing VLAs in task performance and but also exhibits remarkable adaptation to new robots and generalization to unseen objects and backgrounds. It exceeds the average success rates of OpenVLA which has similar model size (7B) with ours by over 35% in simulated evaluation and 55% in real robot experiments. It also outperforms the large RT-2-X model (55B) by 18% absolute success rates in simulation. Code and models can be found on our project page (https://cogact.github.io/).",
      "published_date": "2024-11-29",
      "pdf_url": "https://arxiv.org/pdf/2411.19650.pdf",
      "arxiv_url": "https://arxiv.org/abs/2411.19650",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": "https://github.com/microsoft/CogACT",
      "stars": 374,
      "forks": 33,
      "last_commit": "2025-10-30T08:22:26Z",
      "open_prs": 1,
      "open_issues": 7,
      "watchers": 374,
      "latest_pr_date": "2025-10-30T08:22:26Z"
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "vla"
    ],
    "tags": [
      "VLA",
      "cognition",
      "action",
      "Microsoft"
    ],
    "added_date": "2025-11-13T23:41:11.898690"
  },
  {
    "arxiv": {
      "arxiv_id": "2312.13139",
      "title": "Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation",
      "authors": [
        "Hongtao Wu",
        "Ya Jing",
        "Chilam Cheang",
        "Guangzeng Chen",
        "Jiafeng Xu",
        "Xinghang Li",
        "Minghuan Liu",
        "Hang Li",
        "Tao Kong"
      ],
      "summary": "Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training. We introduce GR-1, a straightforward GPT-style model designed for multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states. It predicts robot actions as well as future images in an end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset. We perform extensive experiments on the challenging CALVIN benchmark and a real robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9% to 94.9%. In the setting of zero-shot unseen scene generalization, GR-1 improves the success rate from 53.3% to 85.4%. In real robot experiments, GR-1 also outperforms baseline methods and shows strong potentials in generalization to unseen scenes and objects. We provide inaugural evidence that a unified GPT-style transformer, augmented with large-scale video generative pre-training, exhibits remarkable generalization to multi-task visual robot manipulation. Project page: https://GR1-Manipulation.github.io",
      "published_date": "2023-12-20",
      "pdf_url": "https://arxiv.org/pdf/2312.13139.pdf",
      "arxiv_url": "https://arxiv.org/abs/2312.13139",
      "categories": [
        "cs.RO",
        "cs.CV"
      ]
    },
    "github": {
      "repo_url": "https://github.com/bytedance/GR-1",
      "stars": 286,
      "forks": 15,
      "last_commit": "2024-04-22T01:42:06Z",
      "open_prs": 0,
      "open_issues": 9,
      "watchers": 286,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "vla"
    ],
    "tags": [
      "VLA",
      "video-generation",
      "ByteDance"
    ],
    "added_date": "2025-11-13T23:41:11.898697"
  },
  {
    "arxiv": {
      "arxiv_id": "2303.03378",
      "title": "PaLM-E: An Embodied Multimodal Language Model",
      "authors": [
        "Danny Driess",
        "Fei Xia",
        "Mehdi S. M. Sajjadi",
        "Corey Lynch",
        "Aakanksha Chowdhery",
        "Brian Ichter",
        "Ayzaan Wahid",
        "Jonathan Tompson",
        "Quan Vuong",
        "Tianhe Yu",
        "Wenlong Huang",
        "Yevgen Chebotar",
        "Pierre Sermanet",
        "Daniel Duckworth",
        "Sergey Levine",
        "Vincent Vanhoucke",
        "Karol Hausman",
        "Marc Toussaint",
        "Klaus Greff",
        "Andy Zeng",
        "Igor Mordatch",
        "Pete Florence"
      ],
      "summary": "Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.",
      "published_date": "2023-03-06",
      "pdf_url": "https://arxiv.org/pdf/2303.03378.pdf",
      "arxiv_url": "https://arxiv.org/abs/2303.03378",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ]
    },
    "github": {
      "repo_url": "https://github.com/kyegomez/PALM-E",
      "stars": 329,
      "forks": 46,
      "last_commit": "2024-01-29T18:47:49Z",
      "open_prs": 0,
      "open_issues": 9,
      "watchers": 329,
      "latest_pr_date": "2023-08-04T11:22:09Z"
    },
    "twitter": {
      "tweet_url": "https://x.com/GoogleAI/status/1634252301303947272",
      "tweet_id": "1634252301303947272",
      "likes": 694,
      "retweets": 210,
      "replies": 17,
      "quotes": 23,
      "views": 173053
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "vla"
    ],
    "tags": [
      "VLA",
      "embodied-LLM",
      "Google"
    ],
    "added_date": "2025-11-13T23:41:11.898703"
  },
  {
    "arxiv": {
      "arxiv_id": "2303.04137",
      "title": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion",
      "authors": [
        "Cheng Chi",
        "Zhenjia Xu",
        "Siyuan Feng",
        "Eric Cousineau",
        "Yilun Du",
        "Benjamin Burchfiel",
        "Russ Tedrake",
        "Shuran Song"
      ],
      "summary": "This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details is publicly available diffusion-policy.cs.columbia.edu",
      "published_date": "2023-03-07",
      "pdf_url": "https://arxiv.org/pdf/2303.04137.pdf",
      "arxiv_url": "https://arxiv.org/abs/2303.04137",
      "categories": [
        "cs.RO"
      ]
    },
    "github": {
      "repo_url": "https://github.com/real-stanford/diffusion_policy",
      "stars": 3338,
      "forks": 612,
      "last_commit": "2024-12-24T19:48:03Z",
      "open_prs": 6,
      "open_issues": 91,
      "watchers": 3338,
      "latest_pr_date": "2025-11-06T09:17:20Z"
    },
    "twitter": {
      "tweet_url": "https://x.com/chichengcc/status/1633339455250526213",
      "tweet_id": "1633339455250526213",
      "likes": 534,
      "retweets": 101,
      "replies": 9,
      "quotes": 20,
      "views": 129017
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "policy_methods"
    ],
    "tags": [
      "diffusion",
      "policy",
      "visuomotor",
      "imitation-learning"
    ],
    "added_date": "2025-11-13T23:41:11.898710"
  },
  {
    "arxiv": {
      "arxiv_id": "2403.03954",
      "title": "3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D Representations",
      "authors": [
        "Yanjie Ze",
        "Gu Zhang",
        "Kangning Zhang",
        "Chenyuan Hu",
        "Muhan Wang",
        "Huazhe Xu"
      ],
      "summary": "Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of human demonstrations. To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models. The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder. In our experiments involving 72 simulation tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 24.2% relative improvement. In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85%, given only 40 demonstrations of each task, and shows excellent generalization abilities in diverse aspects, including space, viewpoint, appearance, and instance. Interestingly, in real robot experiments, DP3 rarely violates safety requirements, in contrast to baseline methods which frequently do, necessitating human intervention. Our extensive evaluation highlights the critical importance of 3D representations in real-world robot learning. Videos, code, and data are available on https://3d-diffusion-policy.github.io .",
      "published_date": "2024-03-06",
      "pdf_url": "https://arxiv.org/pdf/2403.03954.pdf",
      "arxiv_url": "https://arxiv.org/abs/2403.03954",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": "https://github.com/YanjieZe/3D-Diffusion-Policy",
      "stars": 1121,
      "forks": 115,
      "last_commit": "2025-10-17T05:55:25Z",
      "open_prs": 0,
      "open_issues": 3,
      "watchers": 1121,
      "latest_pr_date": "2025-05-29T04:30:53Z"
    },
    "twitter": {
      "tweet_url": "https://x.com/ZeYanjie/status/1765414787775963232",
      "tweet_id": "1765414787775963232",
      "likes": 299,
      "retweets": 58,
      "replies": 4,
      "quotes": 12,
      "views": 96877
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "policy_methods"
    ],
    "tags": [
      "diffusion",
      "3D",
      "policy",
      "point-cloud"
    ],
    "added_date": "2025-11-13T23:41:11.898716"
  },
  {
    "arxiv": {
      "arxiv_id": "2405.07503",
      "title": "Consistency Policy: Accelerated Visuomotor Policies via Consistency Distillation",
      "authors": [
        "Aaditya Prasad",
        "Kevin Lin",
        "Jimmy Wu",
        "Linqi Zhou",
        "Jeannette Bohg"
      ],
      "summary": "Many robotic systems, such as mobile manipulators or quadrotors, cannot be equipped with high-end GPUs due to space, weight, and power constraints. These constraints prevent these systems from leveraging recent developments in visuomotor policy architectures that require high-end GPUs to achieve fast policy inference. In this paper, we propose Consistency Policy, a faster and similarly powerful alternative to Diffusion Policy for learning visuomotor robot control. By virtue of its fast inference speed, Consistency Policy can enable low latency decision making in resource-constrained robotic setups. A Consistency Policy is distilled from a pretrained Diffusion Policy by enforcing self-consistency along the Diffusion Policy's learned trajectories. We compare Consistency Policy with Diffusion Policy and other related speed-up methods across 6 simulation tasks as well as three real-world tasks where we demonstrate inference on a laptop GPU. For all these tasks, Consistency Policy speeds up inference by an order of magnitude compared to the fastest alternative method and maintains competitive success rates. We also show that the Conistency Policy training procedure is robust to the pretrained Diffusion Policy's quality, a useful result that helps practioners avoid extensive testing of the pretrained model. Key design decisions that enabled this performance are the choice of consistency objective, reduced initial sample variance, and the choice of preset chaining steps.",
      "published_date": "2024-05-13",
      "pdf_url": "https://arxiv.org/pdf/2405.07503.pdf",
      "arxiv_url": "https://arxiv.org/abs/2405.07503",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    "github": {
      "repo_url": "https://github.com/Aaditya-Prasad/Consistency-Policy",
      "stars": 189,
      "forks": 14,
      "last_commit": "2024-07-20T06:32:21Z",
      "open_prs": 0,
      "open_issues": 3,
      "watchers": 189,
      "latest_pr_date": "2024-04-24T21:06:54Z"
    },
    "twitter": {
      "tweet_url": "https://x.com/_Aaditya_Prasad/status/1790501613653917782",
      "tweet_id": "1790501613653917782",
      "likes": 291,
      "retweets": 61,
      "replies": 10,
      "quotes": 11,
      "views": 78723
    },
    "semantic_scholar": {
      "paper_id": "289906e335e367d363bc2e99d1c04037da7afbf2",
      "citation_count": 89,
      "influential_citation_count": 11
    },
    "domains": [
      "policy_methods"
    ],
    "tags": [
      "diffusion",
      "policy",
      "consistency",
      "real-time"
    ],
    "added_date": "2025-11-13T23:41:11.898733"
  },
  {
    "arxiv": {
      "arxiv_id": "2407.01479",
      "title": "EquiBot: SIM(3)-Equivariant Diffusion Policy for Generalizable and Data Efficient Learning",
      "authors": [
        "Jingyun Yang",
        "Zi-ang Cao",
        "Congyue Deng",
        "Rika Antonova",
        "Shuran Song",
        "Jeannette Bohg"
      ],
      "summary": "Building effective imitation learning methods that enable robots to learn from limited data and still generalize across diverse real-world environments is a long-standing problem in robot learning. We propose Equibot, a robust, data-efficient, and generalizable approach for robot manipulation task learning. Our approach combines SIM(3)-equivariant neural network architectures with diffusion models. This ensures that our learned policies are invariant to changes in scale, rotation, and translation, enhancing their applicability to unseen environments while retaining the benefits of diffusion-based policy learning such as multi-modality and robustness. We show on a suite of 6 simulation tasks that our proposed method reduces the data requirements and improves generalization to novel scenarios. In the real world, with 10 variations of 6 mobile manipulation tasks, we show that our method can easily generalize to novel objects and scenes after learning from just 5 minutes of human demonstrations in each task.",
      "published_date": "2024-07-01",
      "pdf_url": "https://arxiv.org/pdf/2407.01479.pdf",
      "arxiv_url": "https://arxiv.org/abs/2407.01479",
      "categories": [
        "cs.RO",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": "https://github.com/yjy0625/equibot",
      "stars": 160,
      "forks": 18,
      "last_commit": "2024-07-02T23:08:39Z",
      "open_prs": 0,
      "open_issues": 0,
      "watchers": 160,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": "https://x.com/yjy0625/status/1808550937885356448",
      "tweet_id": "1808550937885356448",
      "likes": 324,
      "retweets": 76,
      "replies": 12,
      "quotes": 14,
      "views": 87804
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "policy_methods"
    ],
    "tags": [
      "diffusion",
      "equivariance",
      "policy",
      "Stanford"
    ],
    "added_date": "2025-11-13T23:41:11.898739"
  },
  {
    "arxiv": {
      "arxiv_id": "2410.03132",
      "title": "Autoregressive Action Sequence Learning for Robotic Manipulation",
      "authors": [
        "Xinyu Zhang",
        "Yuhan Liu",
        "Haonan Chang",
        "Liam Schramm",
        "Abdeslam Boularias"
      ],
      "summary": "Designing a universal policy architecture that performs well across diverse robots and task configurations remains a key challenge. In this work, we address this by representing robot actions as sequential data and generating actions through autoregressive sequence modeling. Existing autoregressive architectures generate end-effector waypoints sequentially as word tokens in language modeling, which are limited to low-frequency control tasks. Unlike language, robot actions are heterogeneous and often include continuous values -- such as joint positions, 2D pixel coordinates, and end-effector poses -- which are not easily suited for language-based modeling. Based on this insight, we introduce a straightforward enhancement: we extend causal transformers' single-token prediction to support predicting a variable number of tokens in a single step through our Chunking Causal Transformer (CCT). This enhancement enables robust performance across diverse tasks of various control frequencies, greater efficiency by having fewer autoregression steps, and lead to a hybrid action sequence design by mixing different types of actions and using a different chunk size for each action type. Based on CCT, we propose the Autoregressive Policy (ARP) architecture, which solves manipulation tasks by generating hybrid action sequences. We evaluate ARP across diverse robotic manipulation environments, including Push-T, ALOHA, and RLBench, and show that ARP, as a universal architecture, matches or outperforms the environment-specific state-of-the-art in all tested benchmarks, while being more efficient in computation and parameter sizes. Videos of our real robot demonstrations, all source code and the pretrained models of ARP can be found at http://github.com/mlzxy/arp.",
      "published_date": "2024-10-04",
      "pdf_url": "https://arxiv.org/pdf/2410.03132.pdf",
      "arxiv_url": "https://arxiv.org/abs/2410.03132",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": "https://github.com/mlzxy/arp",
      "stars": 142,
      "forks": 9,
      "last_commit": "2025-03-25T19:25:02Z",
      "open_prs": 0,
      "open_issues": 9,
      "watchers": 142,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "policy_methods"
    ],
    "tags": [
      "autoregressive",
      "policy",
      "action-chunking"
    ],
    "added_date": "2025-11-13T23:41:11.898745"
  },
  {
    "arxiv": {
      "arxiv_id": "2407.01531",
      "title": "Sparse Diffusion Policy: A Sparse, Reusable, and Flexible Policy for Robot Learning",
      "authors": [
        "Yixiao Wang",
        "Yifei Zhang",
        "Mingxiao Huo",
        "Ran Tian",
        "Xiang Zhang",
        "Yichen Xie",
        "Chenfeng Xu",
        "Pengliang Ji",
        "Wei Zhan",
        "Mingyu Ding",
        "Masayoshi Tomizuka"
      ],
      "summary": "The increasing complexity of tasks in robotics demands efficient strategies for multitask and continual learning. Traditional models typically rely on a universal policy for all tasks, facing challenges such as high computational costs and catastrophic forgetting when learning new tasks. To address these issues, we introduce a sparse, reusable, and flexible policy, Sparse Diffusion Policy (SDP). By adopting Mixture of Experts (MoE) within a transformer-based diffusion policy, SDP selectively activates experts and skills, enabling efficient and task-specific learning without retraining the entire model. SDP not only reduces the burden of active parameters but also facilitates the seamless integration and reuse of experts across various tasks. Extensive experiments on diverse tasks in both simulations and real world show that SDP 1) excels in multitask scenarios with negligible increases in active parameters, 2) prevents forgetting in continual learning of new tasks, and 3) enables efficient task transfer, offering a promising solution for advanced robotic applications. Demos and codes can be found in https://forrest-110.github.io/sparse_diffusion_policy/.",
      "published_date": "2024-07-01",
      "pdf_url": "https://arxiv.org/pdf/2407.01531.pdf",
      "arxiv_url": "https://arxiv.org/abs/2407.01531",
      "categories": [
        "cs.RO",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": "https://github.com/AnthonyHuo/SDP",
      "stars": 71,
      "forks": 7,
      "last_commit": "2024-10-09T01:45:29Z",
      "open_prs": 0,
      "open_issues": 6,
      "watchers": 71,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "policy_methods"
    ],
    "tags": [
      "diffusion",
      "policy",
      "sparse",
      "reusable"
    ],
    "added_date": "2025-11-13T23:41:11.898751"
  },
  {
    "arxiv": {
      "arxiv_id": "2402.10885",
      "title": "3D Diffuser Actor: Policy Diffusion with 3D Scene Representations",
      "authors": [
        "Tsung-Wei Ke",
        "Nikolaos Gkanatsios",
        "Katerina Fragkiadaki"
      ],
      "summary": "Diffusion policies are conditional diffusion models that learn robot action distributions conditioned on the robot and environment state. They have recently shown to outperform both deterministic and alternative action distribution learning formulations. 3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth. They have shown to generalize better than their 2D counterparts across camera viewpoints. We unify these two lines of work and present 3D Diffuser Actor, a neural policy equipped with a novel 3D denoising transformer that fuses information from the 3D visual scene, a language instruction and proprioception to predict the noise in noised 3D robot pose trajectories. 3D Diffuser Actor sets a new state-of-the-art on RLBench with an absolute performance gain of 18.1% over the current SOTA on a multi-view setup and an absolute gain of 13.1% on a single-view setup. On the CALVIN benchmark, it improves over the current SOTA by a 9% relative increase. It also learns to control a robot manipulator in the real world from a handful of demonstrations. Through thorough comparisons with the current SOTA policies and ablations of our model, we show 3D Diffuser Actor's design choices dramatically outperform 2D representations, regression and classification objectives, absolute attentions, and holistic non-tokenized 3D scene embeddings.",
      "published_date": "2024-02-16",
      "pdf_url": "https://arxiv.org/pdf/2402.10885.pdf",
      "arxiv_url": "https://arxiv.org/abs/2402.10885",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": "https://github.com/nickgkan/3d_diffuser_actor",
      "stars": 367,
      "forks": 39,
      "last_commit": "2024-08-17T13:53:49Z",
      "open_prs": 1,
      "open_issues": 3,
      "watchers": 367,
      "latest_pr_date": "2025-08-20T20:26:03Z"
    },
    "twitter": {
      "tweet_url": "https://x.com/nikos_gkanats/status/1759679687520100619",
      "tweet_id": "1759679687520100619",
      "likes": 45,
      "retweets": 11,
      "replies": 1,
      "quotes": 1,
      "views": 4696
    },
    "semantic_scholar": {
      "paper_id": "97fc977b8d167ff648c5c6672aea4d05f98fd79e",
      "citation_count": 208,
      "influential_citation_count": 35
    },
    "domains": [
      "policy_methods"
    ],
    "tags": [
      "diffusion",
      "3D",
      "scene-representation"
    ],
    "added_date": "2025-11-13T23:41:11.898756"
  },
  {
    "arxiv": {
      "arxiv_id": "2305.13301",
      "title": "Training Diffusion Models with Reinforcement Learning",
      "authors": [
        "Kevin Black",
        "Michael Janner",
        "Yilun Du",
        "Ilya Kostrikov",
        "Sergey Levine"
      ],
      "summary": "Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a vision-language model without the need for additional data collection or human annotation. The project's website can be found at http://rl-diffusion.github.io .",
      "published_date": "2023-05-22",
      "pdf_url": "https://arxiv.org/pdf/2305.13301.pdf",
      "arxiv_url": "https://arxiv.org/abs/2305.13301",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    "github": {
      "repo_url": "https://github.com/jannerm/ddpo",
      "stars": 529,
      "forks": 33,
      "last_commit": "2023-07-05T23:07:51Z",
      "open_prs": 0,
      "open_issues": 4,
      "watchers": 529,
      "latest_pr_date": "2023-07-05T23:17:20Z"
    },
    "twitter": {
      "tweet_url": "https://x.com/svlevine/status/1660707076946141184",
      "tweet_id": "1660707076946141184",
      "likes": 823,
      "retweets": 177,
      "replies": 14,
      "quotes": 12,
      "views": 129377
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "policy_methods",
      "rl"
    ],
    "tags": [
      "diffusion",
      "RL",
      "policy",
      "UC-Berkeley"
    ],
    "added_date": "2025-11-13T23:41:11.898762"
  },
  {
    "arxiv": {
      "arxiv_id": "2409.00588",
      "title": "Diffusion Policy Policy Optimization",
      "authors": [
        "Allen Z. Ren",
        "Justin Lidard",
        "Lars L. Ankile",
        "Anthony Simeonov",
        "Pulkit Agrawal",
        "Anirudha Majumdar",
        "Benjamin Burchfiel",
        "Hongkai Dai",
        "Max Simchowitz"
      ],
      "summary": "We introduce Diffusion Policy Policy Optimization, DPPO, an algorithmic framework including best practices for fine-tuning diffusion-based policies (e.g. Diffusion Policy) in continuous control and robot learning tasks using the policy gradient (PG) method from reinforcement learning (RL). PG methods are ubiquitous in training RL policies with other policy parameterizations; nevertheless, they had been conjectured to be less efficient for diffusion-based policies. Surprisingly, we show that DPPO achieves the strongest overall performance and efficiency for fine-tuning in common benchmarks compared to other RL methods for diffusion-based policies and also compared to PG fine-tuning of other policy parameterizations. Through experimental investigation, we find that DPPO takes advantage of unique synergies between RL fine-tuning and the diffusion parameterization, leading to structured and on-manifold exploration, stable training, and strong policy robustness. We further demonstrate the strengths of DPPO in a range of realistic settings, including simulated robotic tasks with pixel observations, and via zero-shot deployment of simulation-trained policies on robot hardware in a long-horizon, multi-stage manipulation task. Website with code: diffusion-ppo.github.io",
      "published_date": "2024-09-01",
      "pdf_url": "https://arxiv.org/pdf/2409.00588.pdf",
      "arxiv_url": "https://arxiv.org/abs/2409.00588",
      "categories": [
        "cs.RO",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": "https://github.com/irom-princeton/dppo",
      "stars": 681,
      "forks": 85,
      "last_commit": "2025-02-04T17:04:46Z",
      "open_prs": 0,
      "open_issues": 23,
      "watchers": 681,
      "latest_pr_date": "2025-03-21T11:14:53Z"
    },
    "twitter": {
      "tweet_url": "https://x.com/allenzren/status/1831403337528570132",
      "tweet_id": "1831403337528570132",
      "likes": 476,
      "retweets": 93,
      "replies": 5,
      "quotes": 12,
      "views": 76173
    },
    "semantic_scholar": {
      "paper_id": "e596c98260ec4096eaeb491eb75f91a8339fcf48",
      "citation_count": 109,
      "influential_citation_count": 16
    },
    "domains": [
      "policy_methods",
      "rl"
    ],
    "tags": [
      "diffusion",
      "RL",
      "PPO",
      "Princeton"
    ],
    "added_date": "2025-11-13T23:41:11.898768"
  },
  {
    "arxiv": {
      "arxiv_id": "2403.04115",
      "title": "DNAct: Diffusion Guided Multi-Task 3D Policy Learning",
      "authors": [
        "Ge Yan",
        "Yueh-Hua Wu",
        "Xiaolong Wang"
      ],
      "summary": "This paper presents DNAct, a language-conditioned multi-task policy framework that integrates neural rendering pre-training and diffusion training to enforce multi-modality learning in action sequence spaces. To learn a generalizable multi-task policy with few demonstrations, the pre-training phase of DNAct leverages neural rendering to distill 2D semantic features from foundation models such as Stable Diffusion to a 3D space, which provides a comprehensive semantic understanding regarding the scene. Consequently, it allows various applications to challenging robotic tasks requiring rich 3D semantics and accurate geometry. Furthermore, we introduce a novel approach utilizing diffusion training to learn a vision and language feature that encapsulates the inherent multi-modality in the multi-task demonstrations. By reconstructing the action sequences from different tasks via the diffusion process, the model is capable of distinguishing different modalities and thus improving the robustness and the generalizability of the learned representation. DNAct significantly surpasses SOTA NeRF-based multi-task manipulation approaches with over 30% improvement in success rate. Project website: dnact.github.io.",
      "published_date": "2024-03-07",
      "pdf_url": "https://arxiv.org/pdf/2403.04115.pdf",
      "arxiv_url": "https://arxiv.org/abs/2403.04115",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ]
    },
    "github": {
      "repo_url": null,
      "stars": null,
      "forks": null,
      "last_commit": null,
      "open_prs": null,
      "open_issues": null,
      "watchers": null,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": "https://x.com/GeYan_21/status/1766323088562786624",
      "tweet_id": "1766323088562786624",
      "likes": 88,
      "retweets": 19,
      "replies": 2,
      "quotes": 4,
      "views": 28802
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "policy_methods"
    ],
    "tags": [
      "diffusion",
      "multi-task",
      "3D"
    ],
    "added_date": "2025-11-13T23:41:11.898772"
  },
  {
    "arxiv": {
      "arxiv_id": "2310.12931",
      "title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
      "authors": [
        "Yecheng Jason Ma",
        "William Liang",
        "Guanzhi Wang",
        "De-An Huang",
        "Osbert Bastani",
        "Dinesh Jayaraman",
        "Yuke Zhu",
        "Linxi Fan",
        "Anima Anandkumar"
      ],
      "summary": "Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.",
      "published_date": "2023-10-19",
      "pdf_url": "https://arxiv.org/pdf/2310.12931.pdf",
      "arxiv_url": "https://arxiv.org/abs/2310.12931",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": "https://github.com/eureka-research/Eureka",
      "stars": 3062,
      "forks": 280,
      "last_commit": "2024-05-03T07:31:13Z",
      "open_prs": 9,
      "open_issues": 38,
      "watchers": 3062,
      "latest_pr_date": "2025-08-21T02:40:18Z"
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": "6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc",
      "citation_count": 429,
      "influential_citation_count": 51
    },
    "domains": [
      "rl"
    ],
    "tags": [
      "RL",
      "LLM",
      "reward-design",
      "NVIDIA"
    ],
    "added_date": "2025-11-13T23:41:11.898778"
  },
  {
    "arxiv": {
      "arxiv_id": "2305.16291",
      "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
      "authors": [
        "Guanzhi Wang",
        "Yuqi Xie",
        "Yunfan Jiang",
        "Ajay Mandlekar",
        "Chaowei Xiao",
        "Yuke Zhu",
        "Linxi Fan",
        "Anima Anandkumar"
      ],
      "summary": "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",
      "published_date": "2023-05-25",
      "pdf_url": "https://arxiv.org/pdf/2305.16291.pdf",
      "arxiv_url": "https://arxiv.org/abs/2305.16291",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": "https://github.com/MineDojo/Voyager",
      "stars": 6454,
      "forks": 614,
      "last_commit": "2024-04-03T18:51:36Z",
      "open_prs": 8,
      "open_issues": 9,
      "watchers": 6454,
      "latest_pr_date": "2025-06-23T18:40:13Z"
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "rl"
    ],
    "tags": [
      "RL",
      "LLM",
      "Minecraft",
      "NVIDIA",
      "open-ended"
    ],
    "added_date": "2025-11-13T23:41:11.898784"
  },
  {
    "arxiv": {
      "arxiv_id": "2509.08827",
      "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
      "authors": [
        "Kaiyan Zhang",
        "Yuxin Zuo",
        "Bingxiang He",
        "Youbang Sun",
        "Runze Liu",
        "Che Jiang",
        "Yuchen Fan",
        "Kai Tian",
        "Guoli Jia",
        "Pengfei Li",
        "Yu Fu",
        "Xingtai Lv",
        "Yuchen Zhang",
        "Sihang Zeng",
        "Shang Qu",
        "Haozhan Li",
        "Shijie Wang",
        "Yuru Wang",
        "Xinwei Long",
        "Fangfu Liu",
        "Xiang Xu",
        "Jiaze Ma",
        "Xuekai Zhu",
        "Ermo Hua",
        "Yihao Liu",
        "Zonglin Li",
        "Huayu Chen",
        "Xiaoye Qu",
        "Yafu Li",
        "Weize Chen",
        "Zhenzhao Yuan",
        "Junqi Gao",
        "Dong Li",
        "Zhiyuan Ma",
        "Ganqu Cui",
        "Zhiyuan Liu",
        "Biqing Qi",
        "Ning Ding",
        "Bowen Zhou"
      ],
      "summary": "In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
      "published_date": "2025-09-10",
      "pdf_url": "https://arxiv.org/pdf/2509.08827.pdf",
      "arxiv_url": "https://arxiv.org/abs/2509.08827",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": "https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
      "stars": 2036,
      "forks": 114,
      "last_commit": "2025-11-09T07:33:16Z",
      "open_prs": 0,
      "open_issues": 0,
      "watchers": 2036,
      "latest_pr_date": "2025-10-18T09:14:12Z"
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "rl"
    ],
    "tags": [
      "RL",
      "LLM",
      "survey",
      "Tsinghua"
    ],
    "added_date": "2025-11-13T23:41:11.898791"
  },
  {
    "arxiv": {
      "arxiv_id": "2402.10329",
      "title": "Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots",
      "authors": [
        "Cheng Chi",
        "Zhenjia Xu",
        "Chuer Pan",
        "Eric Cousineau",
        "Benjamin Burchfiel",
        "Siyuan Feng",
        "Russ Tedrake",
        "Shuran Song"
      ],
      "summary": "We present Universal Manipulation Interface (UMI) -- a data collection and policy learning framework that allows direct skill transfer from in-the-wild human demonstrations to deployable robot policies. UMI employs hand-held grippers coupled with careful interface design to enable portable, low-cost, and information-rich data collection for challenging bimanual and dynamic manipulation demonstrations. To facilitate deployable policy learning, UMI incorporates a carefully designed policy interface with inference-time latency matching and a relative-trajectory action representation. The resulting learned policies are hardware-agnostic and deployable across multiple robot platforms. Equipped with these features, UMI framework unlocks new robot manipulation capabilities, allowing zero-shot generalizable dynamic, bimanual, precise, and long-horizon behaviors, by only changing the training data for each task. We demonstrate UMI's versatility and efficacy with comprehensive real-world experiments, where policies learned via UMI zero-shot generalize to novel environments and objects when trained on diverse human demonstrations. UMI's hardware and software system is open-sourced at https://umi-gripper.github.io.",
      "published_date": "2024-02-15",
      "pdf_url": "https://arxiv.org/pdf/2402.10329.pdf",
      "arxiv_url": "https://arxiv.org/abs/2402.10329",
      "categories": [
        "cs.RO"
      ]
    },
    "github": {
      "repo_url": "https://github.com/real-stanford/universal_manipulation_interface",
      "stars": 1065,
      "forks": 202,
      "last_commit": "2025-07-21T02:19:27Z",
      "open_prs": 1,
      "open_issues": 63,
      "watchers": 1065,
      "latest_pr_date": "2025-07-22T21:01:22Z"
    },
    "twitter": {
      "tweet_url": "https://x.com/chichengcc/status/1758539728444629158",
      "tweet_id": "1758539728444629158",
      "likes": 1790,
      "retweets": 370,
      "replies": 41,
      "quotes": 124,
      "views": 432238
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "data_collection"
    ],
    "tags": [
      "teleoperation",
      "data-collection",
      "UMI",
      "Stanford"
    ],
    "added_date": "2025-11-13T23:41:11.898798"
  },
  {
    "arxiv": {
      "arxiv_id": "2505.21864",
      "title": "DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation",
      "authors": [
        "Mengda Xu",
        "Han Zhang",
        "Yifan Hou",
        "Zhenjia Xu",
        "Linxi Fan",
        "Manuela Veloso",
        "Shuran Song"
      ],
      "summary": "We present DexUMI - a data collection and policy learning framework that uses the human hand as the natural interface to transfer dexterous manipulation skills to various robot hands. DexUMI includes hardware and software adaptations to minimize the embodiment gap between the human hand and various robot hands. The hardware adaptation bridges the kinematics gap using a wearable hand exoskeleton. It allows direct haptic feedback in manipulation data collection and adapts human motion to feasible robot hand motion. The software adaptation bridges the visual gap by replacing the human hand in video data with high-fidelity robot hand inpainting. We demonstrate DexUMI's capabilities through comprehensive real-world experiments on two different dexterous robot hand hardware platforms, achieving an average task success rate of 86%.",
      "published_date": "2025-05-28",
      "pdf_url": "https://arxiv.org/pdf/2505.21864.pdf",
      "arxiv_url": "https://arxiv.org/abs/2505.21864",
      "categories": [
        "cs.RO"
      ]
    },
    "github": {
      "repo_url": "https://github.com/real-stanford/DexUMI",
      "stars": 145,
      "forks": 13,
      "last_commit": "2025-10-02T07:58:55Z",
      "open_prs": 0,
      "open_issues": 3,
      "watchers": 145,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "dexterous",
      "data_collection"
    ],
    "tags": [
      "dexterous",
      "teleoperation",
      "hand",
      "Stanford"
    ],
    "added_date": "2025-11-13T23:41:11.898805"
  },
  {
    "arxiv": {
      "arxiv_id": "2508.00097",
      "title": "XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation",
      "authors": [
        "Zhigen Zhao",
        "Liuchuan Yu",
        "Ke Jing",
        "Ning Yang"
      ],
      "summary": "The rapid advancement of Vision-Language-Action models has created an urgent need for large-scale, high-quality robot demonstration datasets. Although teleoperation is the predominant method for data collection, current approaches suffer from limited scalability, complex setup procedures, and suboptimal data quality. This paper presents XRoboToolkit, a cross-platform framework for extended reality based robot teleoperation built on the OpenXR standard. The system features low-latency stereoscopic visual feedback, optimization-based inverse kinematics, and support for diverse tracking modalities including head, controller, hand, and auxiliary motion trackers. XRoboToolkit's modular architecture enables seamless integration across robotic platforms and simulation environments, spanning precision manipulators, mobile robots, and dexterous hands. We demonstrate the framework's effectiveness through precision manipulation tasks and validate data quality by training VLA models that exhibit robust autonomous performance.",
      "published_date": "2025-07-31",
      "pdf_url": "https://arxiv.org/pdf/2508.00097.pdf",
      "arxiv_url": "https://arxiv.org/abs/2508.00097",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    "github": {
      "repo_url": null,
      "stars": null,
      "forks": null,
      "last_commit": null,
      "open_prs": null,
      "open_issues": null,
      "watchers": null,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "data_collection"
    ],
    "tags": [
      "teleoperation",
      "VR",
      "XR",
      "ByteDance"
    ],
    "added_date": "2025-11-13T23:41:11.898811"
  },
  {
    "arxiv": {
      "arxiv_id": "2503.07771",
      "title": "RoboCopilot: Human-in-the-loop Interactive Imitation Learning for Robot Manipulation",
      "authors": [
        "Philipp Wu",
        "Yide Shentu",
        "Qiayuan Liao",
        "Ding Jin",
        "Menglong Guo",
        "Koushil Sreenath",
        "Xingyu Lin",
        "Pieter Abbeel"
      ],
      "summary": "Learning from human demonstration is an effective approach for learning complex manipulation skills. However, existing approaches heavily focus on learning from passive human demonstration data for its simplicity in data collection. Interactive human teaching has appealing theoretical and practical properties, but they are not well supported by existing human-robot interfaces. This paper proposes a novel system that enables seamless control switching between human and an autonomous policy for bi-manual manipulation tasks, enabling more efficient learning of new tasks. This is achieved through a compliant, bilateral teleoperation system. Through simulation and hardware experiments, we demonstrate the value of our system in an interactive human teaching for learning complex bi-manual manipulation skills.",
      "published_date": "2025-03-10",
      "pdf_url": "https://arxiv.org/pdf/2503.07771.pdf",
      "arxiv_url": "https://arxiv.org/abs/2503.07771",
      "categories": [
        "cs.RO"
      ]
    },
    "github": {
      "repo_url": null,
      "stars": null,
      "forks": null,
      "last_commit": null,
      "open_prs": null,
      "open_issues": null,
      "watchers": null,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": "dcd99528e6f8d554af00f59821c98f9a12858713",
      "citation_count": 12,
      "influential_citation_count": 1
    },
    "domains": [
      "data_collection"
    ],
    "tags": [
      "teleoperation",
      "interactive-learning",
      "RoboCopilot"
    ],
    "added_date": "2025-11-13T23:41:11.898818"
  },
  {
    "arxiv": {
      "arxiv_id": "2403.12945",
      "title": "DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset",
      "authors": [
        "Alexander Khazatsky",
        "Karl Pertsch",
        "Suraj Nair",
        "Ashwin Balakrishna",
        "Sudeep Dasari",
        "Siddharth Karamcheti",
        "Soroush Nasiriany",
        "Mohan Kumar Srirama",
        "Lawrence Yunliang Chen",
        "Kirsty Ellis",
        "Peter David Fagan",
        "Joey Hejna",
        "Masha Itkina",
        "Marion Lepert",
        "Yecheng Jason Ma",
        "Patrick Tree Miller",
        "Jimmy Wu",
        "Suneel Belkhale",
        "Shivin Dass",
        "Huy Ha",
        "Arhan Jain",
        "Abraham Lee",
        "Youngwoon Lee",
        "Marius Memmel",
        "Sungjae Park",
        "Ilija Radosavovic",
        "Kaiyuan Wang",
        "Albert Zhan",
        "Kevin Black",
        "Cheng Chi",
        "Kyle Beltran Hatch",
        "Shan Lin",
        "Jingpei Lu",
        "Jean Mercat",
        "Abdul Rehman",
        "Pannag R Sanketi",
        "Archit Sharma",
        "Cody Simpson",
        "Quan Vuong",
        "Homer Rich Walke",
        "Blake Wulfe",
        "Ted Xiao",
        "Jonathan Heewon Yang",
        "Arefeh Yavary",
        "Tony Z. Zhao",
        "Christopher Agia",
        "Rohan Baijal",
        "Mateo Guaman Castro",
        "Daphne Chen",
        "Qiuyu Chen",
        "Trinity Chung",
        "Jaimyn Drake",
        "Ethan Paul Foster",
        "Jensen Gao",
        "Vitor Guizilini",
        "David Antonio Herrera",
        "Minho Heo",
        "Kyle Hsu",
        "Jiaheng Hu",
        "Muhammad Zubair Irshad",
        "Donovon Jackson",
        "Charlotte Le",
        "Yunshuang Li",
        "Kevin Lin",
        "Roy Lin",
        "Zehan Ma",
        "Abhiram Maddukuri",
        "Suvir Mirchandani",
        "Daniel Morton",
        "Tony Nguyen",
        "Abigail O'Neill",
        "Rosario Scalise",
        "Derick Seale",
        "Victor Son",
        "Stephen Tian",
        "Emi Tran",
        "Andrew E. Wang",
        "Yilin Wu",
        "Annie Xie",
        "Jingyun Yang",
        "Patrick Yin",
        "Yunchu Zhang",
        "Osbert Bastani",
        "Glen Berseth",
        "Jeannette Bohg",
        "Ken Goldberg",
        "Abhinav Gupta",
        "Abhishek Gupta",
        "Dinesh Jayaraman",
        "Joseph J Lim",
        "Jitendra Malik",
        "Roberto Mart\u00edn-Mart\u00edn",
        "Subramanian Ramamoorthy",
        "Dorsa Sadigh",
        "Shuran Song",
        "Jiajun Wu",
        "Michael C. Yip",
        "Yuke Zhu",
        "Thomas Kollar",
        "Sergey Levine",
        "Chelsea Finn"
      ],
      "summary": "The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path toward more capable and robust robotic manipulation policies. However, creating such datasets is challenging: collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial investments in hardware and human labour. As a result, even the most general robot manipulation policies today are mostly trained on data collected in a small number of environments with limited scene and task diversity. In this work, we introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or 350 hours of interaction data, collected across 564 scenes and 84 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance and improved generalization ability. We open source the full dataset, policy learning code, and a detailed guide for reproducing our robot hardware setup.",
      "published_date": "2024-03-19",
      "pdf_url": "https://arxiv.org/pdf/2403.12945.pdf",
      "arxiv_url": "https://arxiv.org/abs/2403.12945",
      "categories": [
        "cs.RO"
      ]
    },
    "github": {
      "repo_url": null,
      "stars": null,
      "forks": null,
      "last_commit": null,
      "open_prs": null,
      "open_issues": null,
      "watchers": null,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "datasets"
    ],
    "tags": [
      "dataset",
      "DROID",
      "in-the-wild",
      "manipulation"
    ],
    "added_date": "2025-11-13T23:41:11.898824"
  },
  {
    "arxiv": {
      "arxiv_id": "2306.03310",
      "title": "LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning",
      "authors": [
        "Bo Liu",
        "Yifeng Zhu",
        "Chongkai Gao",
        "Yihao Feng",
        "Qiang Liu",
        "Yuke Zhu",
        "Peter Stone"
      ],
      "summary": "Lifelong learning offers a promising paradigm of building a generalist agent that learns and adapts over its lifespan. Unlike traditional lifelong learning problems in image and text domains, which primarily involve the transfer of declarative knowledge of entities and concepts, lifelong learning in decision-making (LLDM) also necessitates the transfer of procedural knowledge, such as actions and behaviors. To advance research in LLDM, we introduce LIBERO, a novel benchmark of lifelong learning for robot manipulation. Specifically, LIBERO highlights five key research topics in LLDM: 1) how to efficiently transfer declarative knowledge, procedural knowledge, or the mixture of both; 2) how to design effective policy architectures and 3) effective algorithms for LLDM; 4) the robustness of a lifelong learner with respect to task ordering; and 5) the effect of model pretraining for LLDM. We develop an extendible procedural generation pipeline that can in principle generate infinitely many tasks. For benchmarking purpose, we create four task suites (130 tasks in total) that we use to investigate the above-mentioned research topics. To support sample-efficient learning, we provide high-quality human-teleoperated demonstration data for all tasks. Our extensive experiments present several insightful or even unexpected discoveries: sequential finetuning outperforms existing lifelong learning methods in forward transfer, no single visual encoder architecture excels at all types of knowledge transfer, and naive supervised pretraining can hinder agents' performance in the subsequent LLDM. Check the website at https://libero-project.github.io for the code and the datasets.",
      "published_date": "2023-06-05",
      "pdf_url": "https://arxiv.org/pdf/2306.03310.pdf",
      "arxiv_url": "https://arxiv.org/abs/2306.03310",
      "categories": [
        "cs.AI"
      ]
    },
    "github": {
      "repo_url": "https://github.com/Lifelong-Robot-Learning/LIBERO",
      "stars": 1124,
      "forks": 224,
      "last_commit": "2025-03-15T12:14:04Z",
      "open_prs": 8,
      "open_issues": 73,
      "watchers": 1124,
      "latest_pr_date": "2025-11-04T05:22:38Z"
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": "98cfd7b1b29453c4e82536f5afdc6ddc58bbb1b3",
      "citation_count": 354,
      "influential_citation_count": 109
    },
    "domains": [
      "datasets"
    ],
    "tags": [
      "benchmark",
      "lifelong-learning",
      "knowledge-transfer"
    ],
    "added_date": "2025-11-13T23:41:11.898830"
  },
  {
    "arxiv": {
      "arxiv_id": "2504.03597",
      "title": "Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin",
      "authors": [
        "Jad Abou-Chakra",
        "Lingfeng Sun",
        "Krishan Rana",
        "Brandon May",
        "Karl Schmeckpeper",
        "Niko Suenderhauf",
        "Maria Vittoria Minniti",
        "Laura Herlant"
      ],
      "summary": "We introduce real-is-sim, a new approach to integrating simulation into behavior cloning pipelines. In contrast to real-only methods, which lack the ability to safely test policies before deployment, and sim-to-real methods, which require complex adaptation to cross the sim-to-real gap, our framework allows policies to seamlessly switch between running on real hardware and running in parallelized virtual environments. At the center of real-is-sim is a dynamic digital twin, powered by the Embodied Gaussian simulator, that synchronizes with the real world at 60Hz. This twin acts as a mediator between the behavior cloning policy and the real robot. Policies are trained using representations derived from simulator states and always act on the simulated robot, never the real one. During deployment, the real robot simply follows the simulated robot's joint states, and the simulation is continuously corrected with real world measurements. This setup, where the simulator drives all policy execution and maintains real-time synchronization with the physical world, shifts the responsibility of crossing the sim-to-real gap to the digital twin's synchronization mechanisms, instead of the policy itself. We demonstrate real-is-sim on a long-horizon manipulation task (PushT), showing that virtual evaluations are consistent with real-world results. We further show how real-world data can be augmented with virtual rollouts and compare to policies trained on different representations derived from the simulator state including object poses and rendered images from both static and robot-mounted cameras. Our results highlight the flexibility of the real-is-sim framework across training, evaluation, and deployment stages. Videos available at https://real-is-sim.github.io.",
      "published_date": "2025-04-04",
      "pdf_url": "https://arxiv.org/pdf/2504.03597.pdf",
      "arxiv_url": "https://arxiv.org/abs/2504.03597",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    "github": {
      "repo_url": null,
      "stars": null,
      "forks": null,
      "last_commit": null,
      "open_prs": null,
      "open_issues": null,
      "watchers": null,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "sim_to_real"
    ],
    "tags": [
      "sim-to-real",
      "digital-twin",
      "gaussian-splatting"
    ],
    "added_date": "2025-11-13T23:41:11.898836"
  },
  {
    "arxiv": {
      "arxiv_id": "2304.08488",
      "title": "Affordances from Human Videos as a Versatile Representation for Robotics",
      "authors": [
        "Shikhar Bahl",
        "Russell Mendonca",
        "Lili Chen",
        "Unnat Jain",
        "Deepak Pathak"
      ],
      "summary": "Building a robot that can understand and learn to interact by watching humans has inspired several vision problems. However, despite some successful results on static datasets, it remains unclear how current models can be used on a robot directly. In this paper, we aim to bridge this gap by leveraging videos of human interactions in an environment centric manner. Utilizing internet videos of human behavior, we train a visual affordance model that estimates where and how in the scene a human is likely to interact. The structure of these behavioral affordances directly enables the robot to perform many complex tasks. We show how to seamlessly integrate our affordance model with four robot learning paradigms including offline imitation learning, exploration, goal-conditioned learning, and action parameterization for reinforcement learning. We show the efficacy of our approach, which we call VRB, across 4 real world environments, over 10 different tasks, and 2 robotic platforms operating in the wild. Results, visualizations and videos at https://robo-affordances.github.io/",
      "published_date": "2023-04-17",
      "pdf_url": "https://arxiv.org/pdf/2304.08488.pdf",
      "arxiv_url": "https://arxiv.org/abs/2304.08488",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.NE"
      ]
    },
    "github": {
      "repo_url": null,
      "stars": null,
      "forks": null,
      "last_commit": null,
      "open_prs": null,
      "open_issues": null,
      "watchers": null,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "sim_to_real"
    ],
    "tags": [
      "sim-to-real",
      "affordances",
      "human-videos",
      "cross-embodiment"
    ],
    "added_date": "2025-11-13T23:41:11.898842"
  },
  {
    "arxiv": {
      "arxiv_id": "2311.01455",
      "title": "RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation",
      "authors": [
        "Yufei Wang",
        "Zhou Xian",
        "Feng Chen",
        "Tsun-Hsuan Wang",
        "Yian Wang",
        "Katerina Fragkiadaki",
        "Zackory Erickson",
        "David Held",
        "Chuang Gan"
      ],
      "summary": "We present RoboGen, a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation. RoboGen leverages the latest advancements in foundation and generative models. Instead of directly using or adapting these models to produce policies or low-level actions, we advocate for a generative scheme, which uses these models to automatically generate diversified tasks, scenes, and training supervisions, thereby scaling up robotic skill learning with minimal human supervision. Our approach equips a robotic agent with a self-guided propose-generate-learn cycle: the agent first proposes interesting tasks and skills to develop, and then generates corresponding simulation environments by populating pertinent objects and assets with proper spatial configurations. Afterwards, the agent decomposes the proposed high-level task into sub-tasks, selects the optimal learning approach (reinforcement learning, motion planning, or trajectory optimization), generates required training supervision, and then learns policies to acquire the proposed skill. Our work attempts to extract the extensive and versatile knowledge embedded in large-scale models and transfer them to the field of robotics. Our fully generative pipeline can be queried repeatedly, producing an endless stream of skill demonstrations associated with diverse tasks and environments.",
      "published_date": "2023-11-02",
      "pdf_url": "https://arxiv.org/pdf/2311.01455.pdf",
      "arxiv_url": "https://arxiv.org/abs/2311.01455",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": "https://github.com/robogen-ai/robogen",
      "stars": null,
      "forks": null,
      "last_commit": null,
      "open_prs": null,
      "open_issues": null,
      "watchers": null,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "sim_to_real"
    ],
    "tags": [
      "sim-to-real",
      "procedural-generation",
      "RoboGen"
    ],
    "added_date": "2025-11-13T23:41:11.898849"
  },
  {
    "arxiv": {
      "arxiv_id": "2406.16862",
      "title": "Dreamitate: Real-World Visuomotor Policy Learning via Video Generation",
      "authors": [
        "Junbang Liang",
        "Ruoshi Liu",
        "Ege Ozguroglu",
        "Sruthi Sudhakar",
        "Achal Dave",
        "Pavel Tokmakov",
        "Shuran Song",
        "Carl Vondrick"
      ],
      "summary": "A key challenge in manipulation is learning a policy that can robustly generalize to diverse visual environments. A promising mechanism for learning robust policies is to leverage video generative models, which are pretrained on large-scale datasets of internet videos. In this paper, we propose a visuomotor policy learning framework that fine-tunes a video diffusion model on human demonstrations of a given task. At test time, we generate an example of an execution of the task conditioned on images of a novel scene, and use this synthesized execution directly to control the robot. Our key insight is that using common tools allows us to effortlessly bridge the embodiment gap between the human hand and the robot manipulator. We evaluate our approach on four tasks of increasing complexity and demonstrate that harnessing internet-scale generative models allows the learned policy to achieve a significantly higher degree of generalization than existing behavior cloning approaches.",
      "published_date": "2024-06-24",
      "pdf_url": "https://arxiv.org/pdf/2406.16862.pdf",
      "arxiv_url": "https://arxiv.org/abs/2406.16862",
      "categories": [
        "cs.RO",
        "cs.CV"
      ]
    },
    "github": {
      "repo_url": null,
      "stars": null,
      "forks": null,
      "last_commit": null,
      "open_prs": null,
      "open_issues": null,
      "watchers": null,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": "b0ac4f62f55bcf0427008e18f1b4b5bf7ee43df2",
      "citation_count": 49,
      "influential_citation_count": 3
    },
    "domains": [
      "sim_to_real"
    ],
    "tags": [
      "sim-to-real",
      "video-generation",
      "diffusion",
      "Dreamitate"
    ],
    "added_date": "2025-11-13T23:41:11.898855"
  },
  {
    "arxiv": {
      "arxiv_id": "2312.08344",
      "title": "FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects",
      "authors": [
        "Bowen Wen",
        "Wei Yang",
        "Jan Kautz",
        "Stan Birchfield"
      ],
      "summary": "We present FoundationPose, a unified foundation model for 6D object pose estimation and tracking, supporting both model-based and model-free setups. Our approach can be instantly applied at test-time to a novel object without fine-tuning, as long as its CAD model is given, or a small number of reference images are captured. We bridge the gap between these two setups with a neural implicit representation that allows for effective novel view synthesis, keeping the downstream pose estimation modules invariant under the same unified framework. Strong generalizability is achieved via large-scale synthetic training, aided by a large language model (LLM), a novel transformer-based architecture, and contrastive learning formulation. Extensive evaluation on multiple public datasets involving challenging scenarios and objects indicate our unified approach outperforms existing methods specialized for each task by a large margin. In addition, it even achieves comparable results to instance-level methods despite the reduced assumptions. Project page: https://nvlabs.github.io/FoundationPose/",
      "published_date": "2023-12-13",
      "pdf_url": "https://arxiv.org/pdf/2312.08344.pdf",
      "arxiv_url": "https://arxiv.org/abs/2312.08344",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ]
    },
    "github": {
      "repo_url": null,
      "stars": null,
      "forks": null,
      "last_commit": null,
      "open_prs": null,
      "open_issues": null,
      "watchers": null,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "perception"
    ],
    "tags": [
      "pose-estimation",
      "6D",
      "foundation-model",
      "NVIDIA"
    ],
    "added_date": "2025-11-13T23:41:11.898862"
  },
  {
    "arxiv": {
      "arxiv_id": "2410.09309",
      "title": "Adaptive Compliance Policy: Learning Approximate Compliance for Diffusion Guided Control",
      "authors": [
        "Yifan Hou",
        "Zeyi Liu",
        "Cheng Chi",
        "Eric Cousineau",
        "Naveen Kuppuswamy",
        "Siyuan Feng",
        "Benjamin Burchfiel",
        "Shuran Song"
      ],
      "summary": "Compliance plays a crucial role in manipulation, as it balances between the concurrent control of position and force under uncertainties. Yet compliance is often overlooked by today's visuomotor policies that solely focus on position control. This paper introduces Adaptive Compliance Policy (ACP), a novel framework that learns to dynamically adjust system compliance both spatially and temporally for given manipulation tasks from human demonstrations, improving upon previous approaches that rely on pre-selected compliance parameters or assume uniform constant stiffness. However, computing full compliance parameters from human demonstrations is an ill-defined problem. Instead, we estimate an approximate compliance profile with two useful properties: avoiding large contact forces and encouraging accurate tracking. Our approach enables robots to handle complex contact-rich manipulation tasks and achieves over 50\\% performance improvement compared to state-of-the-art visuomotor policy methods. For result videos, see https://adaptive-compliance.github.io/",
      "published_date": "2024-10-12",
      "pdf_url": "https://arxiv.org/pdf/2410.09309.pdf",
      "arxiv_url": "https://arxiv.org/abs/2410.09309",
      "categories": [
        "cs.RO"
      ]
    },
    "github": {
      "repo_url": "https://github.com/yifan-hou/adaptive_compliance_policy",
      "stars": 92,
      "forks": 8,
      "last_commit": "2024-10-23T17:20:27Z",
      "open_prs": 0,
      "open_issues": 1,
      "watchers": 92,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "manipulation"
    ],
    "tags": [
      "compliance",
      "diffusion",
      "contact-rich",
      "Stanford"
    ],
    "added_date": "2025-11-13T23:41:11.898869"
  },
  {
    "arxiv": {
      "arxiv_id": "2210.03173",
      "title": "CoGrasp: 6-DoF Grasp Generation for Human-Robot Collaboration",
      "authors": [
        "Abhinav K. Keshari",
        "Hanwen Ren",
        "Ahmed H. Qureshi"
      ],
      "summary": "Robot grasping is an actively studied area in robotics, mainly focusing on the quality of generated grasps for object manipulation. However, despite advancements, these methods do not consider the human-robot collaboration settings where robots and humans will have to grasp the same objects concurrently. Therefore, generating robot grasps compatible with human preferences of simultaneously holding an object becomes necessary to ensure a safe and natural collaboration experience. In this paper, we propose a novel, deep neural network-based method called CoGrasp that generates human-aware robot grasps by contextualizing human preference models of object grasping into the robot grasp selection process. We validate our approach against existing state-of-the-art robot grasping methods through simulated and real-robot experiments and user studies. In real robot experiments, our method achieves about 88\\% success rate in producing stable grasps that also allow humans to interact and grasp objects simultaneously in a socially compliant manner. Furthermore, our user study with 10 independent participants indicated our approach enables a safe, natural, and socially-aware human-robot objects' co-grasping experience compared to a standard robot grasping technique.",
      "published_date": "2022-10-06",
      "pdf_url": "https://arxiv.org/pdf/2210.03173.pdf",
      "arxiv_url": "https://arxiv.org/abs/2210.03173",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": null,
      "stars": null,
      "forks": null,
      "last_commit": null,
      "open_prs": null,
      "open_issues": null,
      "watchers": null,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "manipulation",
      "dexterous"
    ],
    "tags": [
      "grasping",
      "6-DoF",
      "collaboration",
      "CoGrasp"
    ],
    "added_date": "2025-11-13T23:41:11.898875"
  },
  {
    "arxiv": {
      "arxiv_id": "2504.13165",
      "title": "RUKA: Rethinking the Design of Humanoid Hands with Learning",
      "authors": [
        "Anya Zorin",
        "Irmak Guzey",
        "Billy Yan",
        "Aadhithya Iyer",
        "Lisa Kondrich",
        "Nikhil X. Bhattasali",
        "Lerrel Pinto"
      ],
      "summary": "Dexterous manipulation is a fundamental capability for robotic systems, yet progress has been limited by hardware trade-offs between precision, compactness, strength, and affordability. Existing control methods impose compromises on hand designs and applications. However, learning-based approaches present opportunities to rethink these trade-offs, particularly to address challenges with tendon-driven actuation and low-cost materials. This work presents RUKA, a tendon-driven humanoid hand that is compact, affordable, and capable. Made from 3D-printed parts and off-the-shelf components, RUKA has 5 fingers with 15 underactuated degrees of freedom enabling diverse human-like grasps. Its tendon-driven actuation allows powerful grasping in a compact, human-sized form factor. To address control challenges, we learn joint-to-actuator and fingertip-to-actuator models from motion-capture data collected by the MANUS glove, leveraging the hand's morphological accuracy. Extensive evaluations demonstrate RUKA's superior reachability, durability, and strength compared to other robotic hands. Teleoperation tasks further showcase RUKA's dexterous movements. The open-source design and assembly instructions of RUKA, code, and data are available at https://ruka-hand.github.io/.",
      "published_date": "2025-04-17",
      "pdf_url": "https://arxiv.org/pdf/2504.13165.pdf",
      "arxiv_url": "https://arxiv.org/abs/2504.13165",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    "github": {
      "repo_url": "https://github.com/ruka-hand/RUKA",
      "stars": 153,
      "forks": 20,
      "last_commit": "2025-08-11T20:18:48Z",
      "open_prs": 1,
      "open_issues": 3,
      "watchers": 153,
      "latest_pr_date": "2025-10-09T02:55:34Z"
    },
    "twitter": {
      "tweet_url": "https://x.com/irmakkguzey/status/1913276064287305730",
      "tweet_id": "1913276064287305730",
      "likes": 445,
      "retweets": 100,
      "replies": 17,
      "quotes": 27,
      "views": 109358
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "dexterous",
      "manipulation"
    ],
    "tags": [
      "dexterous",
      "hand",
      "hardware",
      "Stanford"
    ],
    "added_date": "2025-11-13T23:41:11.898880"
  },
  {
    "arxiv": {
      "arxiv_id": "2307.05973",
      "title": "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models",
      "authors": [
        "Wenlong Huang",
        "Chen Wang",
        "Ruohan Zhang",
        "Yunzhu Li",
        "Jiajun Wu",
        "Li Fei-Fei"
      ],
      "summary": "Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Videos and code at https://voxposer.github.io",
      "published_date": "2023-07-12",
      "pdf_url": "https://arxiv.org/pdf/2307.05973.pdf",
      "arxiv_url": "https://arxiv.org/abs/2307.05973",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": null,
      "stars": null,
      "forks": null,
      "last_commit": null,
      "open_prs": null,
      "open_issues": null,
      "watchers": null,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": "1cd8373490efc2d74c2796f4b2aa27c7d4415ec9",
      "citation_count": 674,
      "influential_citation_count": 54
    },
    "domains": [
      "manipulation",
      "hri_planning"
    ],
    "tags": [
      "LLM",
      "manipulation",
      "3D",
      "value-maps",
      "Google"
    ],
    "added_date": "2025-11-13T23:41:11.898886"
  },
  {
    "arxiv": {
      "arxiv_id": "2305.05658",
      "title": "TidyBot: Personalized Robot Assistance with Large Language Models",
      "authors": [
        "Jimmy Wu",
        "Rika Antonova",
        "Adam Kan",
        "Marion Lepert",
        "Andy Zeng",
        "Shuran Song",
        "Jeannette Bohg",
        "Szymon Rusinkiewicz",
        "Thomas Funkhouser"
      ],
      "summary": "For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accuracy on unseen objects in our benchmark dataset. We also demonstrate our approach on a real-world mobile manipulator called TidyBot, which successfully puts away 85.0% of objects in real-world test scenarios.",
      "published_date": "2023-05-09",
      "pdf_url": "https://arxiv.org/pdf/2305.05658.pdf",
      "arxiv_url": "https://arxiv.org/abs/2305.05658",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": "https://github.com/jimmyyhwu/tidybot",
      "stars": 666,
      "forks": 84,
      "last_commit": "2023-11-10T16:28:38Z",
      "open_prs": 0,
      "open_issues": 0,
      "watchers": 666,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "mobile_manipulation",
      "hri_planning"
    ],
    "tags": [
      "mobile-manipulation",
      "LLM",
      "planning",
      "Stanford"
    ],
    "added_date": "2025-11-13T23:41:11.898892"
  },
  {
    "arxiv": {
      "arxiv_id": "2310.07896",
      "title": "NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration",
      "authors": [
        "Ajay Sridhar",
        "Dhruv Shah",
        "Catherine Glossop",
        "Sergey Levine"
      ],
      "summary": "Robotic learning for navigation in unfamiliar environments needs to provide policies for both task-oriented navigation (i.e., reaching a goal that the robot has located), and task-agnostic exploration (i.e., searching for a goal in a novel setting). Typically, these roles are handled by separate models, for example by using subgoal proposals, planning, or separate navigation strategies. In this paper, we describe how we can train a single unified diffusion policy to handle both goal-directed navigation and goal-agnostic exploration, with the latter providing the ability to search novel environments, and the former providing the ability to reach a user-specified goal once it has been located. We show that this unified policy results in better overall performance when navigating to visually indicated goals in novel environments, as compared to approaches that use subgoal proposals from generative models, or prior methods based on latent variable models. We instantiate our method by using a large-scale Transformer-based policy trained on data from multiple ground robots, with a diffusion model decoder to flexibly handle both goal-conditioned and goal-agnostic navigation. Our experiments, conducted on a real-world mobile robot platform, show effective navigation in unseen environments in comparison with five alternative methods, and demonstrate significant improvements in performance and lower collision rates, despite utilizing smaller models than state-of-the-art approaches. For more videos, code, and pre-trained model checkpoints, see https://general-navigation-models.github.io/nomad/",
      "published_date": "2023-10-11",
      "pdf_url": "https://arxiv.org/pdf/2310.07896.pdf",
      "arxiv_url": "https://arxiv.org/abs/2310.07896",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": null,
      "stars": null,
      "forks": null,
      "last_commit": null,
      "open_prs": null,
      "open_issues": null,
      "watchers": null,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "mobile_manipulation"
    ],
    "tags": [
      "navigation",
      "diffusion",
      "exploration",
      "NoMaD"
    ],
    "added_date": "2025-11-13T23:41:11.898898"
  },
  {
    "arxiv": {
      "arxiv_id": "2304.07193",
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "authors": [
        "Maxime Oquab",
        "Timoth\u00e9e Darcet",
        "Th\u00e9o Moutakanni",
        "Huy Vo",
        "Marc Szafraniec",
        "Vasil Khalidov",
        "Pierre Fernandez",
        "Daniel Haziza",
        "Francisco Massa",
        "Alaaeldin El-Nouby",
        "Mahmoud Assran",
        "Nicolas Ballas",
        "Wojciech Galuba",
        "Russell Howes",
        "Po-Yao Huang",
        "Shang-Wen Li",
        "Ishan Misra",
        "Michael Rabbat",
        "Vasu Sharma",
        "Gabriel Synnaeve",
        "Hu Xu",
        "Herv\u00e9 Jegou",
        "Julien Mairal",
        "Patrick Labatut",
        "Armand Joulin",
        "Piotr Bojanowski"
      ],
      "summary": "The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.",
      "published_date": "2023-04-14",
      "pdf_url": "https://arxiv.org/pdf/2304.07193.pdf",
      "arxiv_url": "https://arxiv.org/abs/2304.07193",
      "categories": [
        "cs.CV"
      ]
    },
    "github": {
      "repo_url": null,
      "stars": null,
      "forks": null,
      "last_commit": null,
      "open_prs": null,
      "open_issues": null,
      "watchers": null,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "perception"
    ],
    "tags": [
      "vision",
      "foundation-model",
      "DINOv2",
      "Meta"
    ],
    "added_date": "2025-11-13T23:41:11.898910"
  },
  {
    "arxiv": {
      "arxiv_id": "2209.07753",
      "title": "Code as Policies: Language Model Programs for Embodied Control",
      "authors": [
        "Jacky Liang",
        "Wenlong Huang",
        "Fei Xia",
        "Peng Xu",
        "Karol Hausman",
        "Brian Ichter",
        "Pete Florence",
        "Andy Zeng"
      ],
      "summary": "Large language models (LLMs) trained on code completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g.,from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (\"faster\") depending on context (i.e., behavioral commonsense). This paper presents code as policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io",
      "published_date": "2022-09-16",
      "pdf_url": "https://arxiv.org/pdf/2209.07753.pdf",
      "arxiv_url": "https://arxiv.org/abs/2209.07753",
      "categories": [
        "cs.RO"
      ]
    },
    "github": {
      "repo_url": null,
      "stars": null,
      "forks": null,
      "last_commit": null,
      "open_prs": null,
      "open_issues": null,
      "watchers": null,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": "91deaf9d324c8feafc189da0da03e60a60287bca",
      "citation_count": 1178,
      "influential_citation_count": 93
    },
    "domains": [
      "hri_planning"
    ],
    "tags": [
      "LLM",
      "planning",
      "code-generation",
      "Google"
    ],
    "added_date": "2025-11-13T23:41:11.898916"
  },
  {
    "arxiv": {
      "arxiv_id": "2209.11302",
      "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models",
      "authors": [
        "Ishika Singh",
        "Valts Blukis",
        "Arsalan Mousavian",
        "Ankit Goyal",
        "Danfei Xu",
        "Jonathan Tremblay",
        "Dieter Fox",
        "Jesse Thomason",
        "Animesh Garg"
      ],
      "summary": "Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website at progprompt.github.io",
      "published_date": "2022-09-22",
      "pdf_url": "https://arxiv.org/pdf/2209.11302.pdf",
      "arxiv_url": "https://arxiv.org/abs/2209.11302",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": null,
      "stars": null,
      "forks": null,
      "last_commit": null,
      "open_prs": null,
      "open_issues": null,
      "watchers": null,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": "c03fa01fbb9c77fe3d10609ba5f1dee33a723867",
      "citation_count": 799,
      "influential_citation_count": 48
    },
    "domains": [
      "hri_planning"
    ],
    "tags": [
      "LLM",
      "planning",
      "prompting",
      "ProgPrompt"
    ],
    "added_date": "2025-11-13T23:41:11.898921"
  },
  {
    "arxiv": {
      "arxiv_id": "2306.06531",
      "title": "AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers",
      "authors": [
        "Yongchao Chen",
        "Jacob Arkin",
        "Charles Dawson",
        "Yang Zhang",
        "Nicholas Roy",
        "Chuchu Fan"
      ],
      "summary": "For effective human-robot interaction, robots need to understand, plan, and execute complex, long-horizon tasks described by natural language. Recent advances in large language models (LLMs) have shown promise for translating natural language into robot action sequences for complex tasks. However, existing approaches either translate the natural language directly into robot trajectories or factor the inference process by decomposing language into task sub-goals and relying on a motion planner to execute each sub-goal. When complex environmental and temporal constraints are involved, inference over planning tasks must be performed jointly with motion plans using traditional task-and-motion planning (TAMP) algorithms, making factorization into subgoals untenable. Rather than using LLMs to directly plan task sub-goals, we instead perform few-shot translation from natural language task descriptions to an intermediate task representation that can then be consumed by a TAMP algorithm to jointly solve the task and motion plan. To improve translation, we automatically detect and correct both syntactic and semantic errors via autoregressive re-prompting, resulting in significant improvements in task completion. We show that our approach outperforms several methods using LLMs as planners in complex task domains. See our project website https://yongchao98.github.io/MIT-REALM-AutoTAMP/ for prompts, videos, and code.",
      "published_date": "2023-06-10",
      "pdf_url": "https://arxiv.org/pdf/2306.06531.pdf",
      "arxiv_url": "https://arxiv.org/abs/2306.06531",
      "categories": [
        "cs.RO",
        "cs.CL",
        "cs.HC"
      ]
    },
    "github": {
      "repo_url": null,
      "stars": null,
      "forks": null,
      "last_commit": null,
      "open_prs": null,
      "open_issues": null,
      "watchers": null,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "hri_planning"
    ],
    "tags": [
      "LLM",
      "TAMP",
      "planning",
      "AutoTAMP"
    ],
    "added_date": "2025-11-13T23:41:11.898926"
  },
  {
    "arxiv": {
      "arxiv_id": "2307.04738",
      "title": "RoCo: Dialectic Multi-Robot Collaboration with Large Language Models",
      "authors": [
        "Zhao Mandi",
        "Shreeya Jain",
        "Shuran Song"
      ],
      "summary": "We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They then generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset for agent representation and reasoning. We experimentally demonstrate the effectiveness of our approach -- it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility -- in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together. See project website https://project-roco.github.io for videos and code.",
      "published_date": "2023-07-10",
      "pdf_url": "https://arxiv.org/pdf/2307.04738.pdf",
      "arxiv_url": "https://arxiv.org/abs/2307.04738",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ]
    },
    "github": {
      "repo_url": null,
      "stars": null,
      "forks": null,
      "last_commit": null,
      "open_prs": null,
      "open_issues": null,
      "watchers": null,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "hri_planning"
    ],
    "tags": [
      "LLM",
      "multi-robot",
      "collaboration",
      "RoCo"
    ],
    "added_date": "2025-11-13T23:41:11.898931"
  },
  {
    "arxiv": {
      "arxiv_id": "2501.00785",
      "title": "Natural Multimodal Fusion-Based Human-Robot Interaction: Application With Voice and Deictic Posture via Large Language Model",
      "authors": [
        "Yuzhi Lai",
        "Shenghai Yuan",
        "Youssef Nassar",
        "Mingyu Fan",
        "Atmaraaj Gopal",
        "Arihiro Yorita",
        "Naoyuki Kubota",
        "Matthias R\u00e4tsch"
      ],
      "summary": "Translating human intent into robot commands is crucial for the future of service robots in an aging society. Existing Human-Robot Interaction (HRI) systems relying on gestures or verbal commands are impractical for the elderly due to difficulties with complex syntax or sign language. To address the challenge, this paper introduces a multi-modal interaction framework that combines voice and deictic posture information to create a more natural HRI system. The visual cues are first processed by the object detection model to gain a global understanding of the environment, and then bounding boxes are estimated based on depth information. By using a large language model (LLM) with voice-to-text commands and temporally aligned selected bounding boxes, robot action sequences can be generated, while key control syntax constraints are applied to avoid potential LLM hallucination issues. The system is evaluated on real-world tasks with varying levels of complexity using a Universal Robots UR3e manipulator. Our method demonstrates significantly better performance in HRI in terms of accuracy and robustness. To benefit the research community and the general public, we will make our code and design open-source.",
      "published_date": "2025-01-01",
      "pdf_url": "https://arxiv.org/pdf/2501.00785.pdf",
      "arxiv_url": "https://arxiv.org/abs/2501.00785",
      "categories": [
        "cs.RO"
      ]
    },
    "github": {
      "repo_url": null,
      "stars": null,
      "forks": null,
      "last_commit": null,
      "open_prs": null,
      "open_issues": null,
      "watchers": null,
      "latest_pr_date": null
    },
    "twitter": {
      "tweet_url": null,
      "tweet_id": null,
      "likes": null,
      "retweets": null,
      "replies": null,
      "quotes": null,
      "views": null
    },
    "semantic_scholar": {
      "paper_id": null,
      "citation_count": null,
      "influential_citation_count": null
    },
    "domains": [
      "hri_planning"
    ],
    "tags": [
      "HRI",
      "multimodal",
      "LLM",
      "gestures"
    ],
    "added_date": "2025-11-13T23:41:11.898935"
  }
]