# Foundation Models & VLAs

Vision-Language-Action models and foundation models for robotics. These models combine vision, language, and action to enable versatile robotic manipulation and decision-making.

**Total Papers**: 10

---

## Papers

| Paper | arXiv | Date | GitHub | Stars | Citations | Twitter | Summary |
|-------|-------|------|--------|-------|-----------|---------|----------|
| [Open X-Embodiment: Robotic Learning...](https://arxiv.org/abs/2310.08864)<br>_Open X-Embodiment Collaboration, Abby O'Neill et al._ | 2310.08864 | 2023-10-13 | - | - | - | [â¤ï¸ 1,173 ğŸ”„ 302](https://x.com/GoogleDeepMind/status/1709207886943965648)<br>ğŸ‘ï¸ 518,773 | Large, high-capacity models trained on diverse datasets have shown remarkable successes on effici... |
| [GR00T N1: An Open Foundation Model ...](https://arxiv.org/abs/2503.14734)<br>_ NVIDIA,  : et al._ | 2503.14734 | 2025-03-18 | - | - | - | [â¤ï¸ 1,945 ğŸ”„ 398](https://x.com/DrJimFan/status/1902117478496616642)<br>ğŸ‘ï¸ 450,145 | General-purpose robots need a versatile body and an intelligent mind. |
| [OpenVLA: An Open-Source Vision-Lang...](https://arxiv.org/abs/2406.09246)<br>_Moo Jin Kim, Karl Pertsch et al._ | 2406.09246 | 2024-06-13 | [repo](https://github.com/openvla/openvla) | 4,415 | - | [â¤ï¸ 695 ğŸ”„ 162](https://x.com/moo_jin_kim/status/1801548441102991771)<br>ğŸ‘ï¸ 226,126 | Large policies pretrained on a combination of Internet-scale vision-language data and diverse rob... |
| [RT-2: Vision-Language-Action Models...](https://arxiv.org/abs/2307.15818)<br>_Anthony Brohan, Noah Brown et al._ | 2307.15818 | 2023-07-28 | [repo](https://github.com/kyegomez/RT-2) | 528 | - | [â¤ï¸ 1,590 ğŸ”„ 435](https://x.com/GoogleDeepMind/status/1684903412834447360)<br>ğŸ‘ï¸ 537,063 | We study how vision-language models trained on Internet-scale data can be incorporated directly i... |
| [CogACT: A Foundational Vision-Langu...](https://arxiv.org/abs/2411.19650)<br>_Qixiu Li, Yaobo Liang et al._ | 2411.19650 | 2024-11-29 | [repo](https://github.com/microsoft/CogACT) | 374 | - | - | The advancement of large Vision-Language-Action (VLA) models has significantly improved robotic m... |
| [Unleashing Large-Scale Video Genera...](https://arxiv.org/abs/2312.13139)<br>_Hongtao Wu, Ya Jing et al._ | 2312.13139 | 2023-12-20 | [repo](https://github.com/bytedance/GR-1) | 286 | - | - | Generative pre-trained models have demonstrated remarkable effectiveness in language and vision d... |
| [PaLM-E: An Embodied Multimodal Lang...](https://arxiv.org/abs/2303.03378)<br>_Danny Driess, Fei Xia et al._ | 2303.03378 | 2023-03-06 | [repo](https://github.com/kyegomez/PALM-E) | 329 | - | [â¤ï¸ 694 ğŸ”„ 210](https://x.com/GoogleAI/status/1634252301303947272)<br>ğŸ‘ï¸ 173,053 | Large language models excel at a wide range of complex tasks. |
| [An Embodied Generalist Agent in 3D ...](https://arxiv.org/abs/2311.12871)<br>_Jiangyong Huang, Silong Yong et al._ | 2311.12871 | 2023-11-18 | [repo](https://github.com/embodied-generalist/embodied-generalist) | 465 | - | [â¤ï¸ 199 ğŸ”„ 34](https://x.com/jeasinema/status/1727595460867862930)<br>ğŸ‘ï¸ 81,872 | Leveraging massive knowledge from large language models (LLMs), recent machine learning models sh... |
| [VIMA: General Robot Manipulation wi...](https://arxiv.org/abs/2210.03094)<br>_Yunfan Jiang, Agrim Gupta et al._ | 2210.03094 | 2022-10-06 | [repo](https://github.com/vimalabs/VIMA) | 835 | - | [â¤ï¸ 856 ğŸ”„ 147](https://x.com/DrJimFan/status/1578433493561769984) | Prompt-based learning has emerged as a successful paradigm in natural language processing, where ... |
| [AHA: A Vision-Language-Model for De...](https://arxiv.org/abs/2410.00371)<br>_Jiafei Duan, Wilbert Pumacay et al._ | 2410.00371 | 2024-10-01 | [repo](https://github.com/NVlabs/AHA) | 49 | - | [â¤ï¸ 201 ğŸ”„ 43](https://x.com/DJiafei/status/1838562171460161619)<br>ğŸ‘ï¸ 48,462 | Robotic manipulation in open-world settings requires not only task execution but also the ability... |


---

_Last updated: 2025-11-13_
