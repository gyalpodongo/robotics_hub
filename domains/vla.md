# Foundation Models & VLAs

Vision-Language-Action (VLA) models represent a paradigm shift in robotics by combining visual perception, natural language understanding, and action prediction in a single unified model. These foundation models leverage large-scale pre-training on internet data and robot demonstrations to enable general-purpose robotic manipulation.

## What are VLA Models?

VLA models are transformer-based architectures that process visual observations and language instructions to generate low-level robot actions. They bridge the gap between high-level semantic understanding (from vision-language models) and low-level control (for robot actuation), enabling robots to follow natural language commands in diverse environments.

## Key Approaches

### End-to-End VLAs
Models like **OpenVLA** and **RT-2** directly map pixels and language to actions using large transformers trained on millions of robot demonstrations.

### Embodied Multimodal Models
Models like **PaLM-E** inject embodied sensor data into large language models, allowing them to reason about the physical world.

### Cross-Embodiment Transfer
The **Open X-Embodiment** dataset enables training policies that generalize across different robot morphologies.

## Important Considerations

- **Data scale**: VLAs require millions of diverse demonstrations
- **Compute requirements**: Training demands significant GPU resources
- **Sim-to-real gap**: Real-world fine-tuning is essential
- **Safety & robustness**: Verification is critical

## Notable Models & Datasets

- **OpenVLA**: 7B-parameter open-source VLA
- **RT-2**: Google's 55B-parameter VLA
- **Open X-Embodiment**: 1M+ demonstration dataset


---

## ğŸ”¥ Latest Trends & Research Directions

*Last Updated: 2025-11-14 00:33 UTC*

### 1. Emerging Techniques

*   **Diffusion Models for Action Generation:** Integrating diffusion models, particularly Diffusion Transformers (DiT), into VLAs is a prominent emerging technique. CogACT and GROOT N1 leverage DiTs for action generation, improving action precision and handling the continuous, multimodal, and temporally correlated nature of robot actions. GROOT N1 utilizes flow-matching for enhanced action generation.
*   **Dual-System Architectures:** The use of dual-system compositional architectures, inspired by human cognition (as seen in GROOT N1), is gaining traction. This involves separating high-level reasoning (System 2, often a VLM) from low-level action generation (System 1, often a DiT), allowing specialized optimization of each component.
*   **Adaptive Action Ensembles:** Adaptive action ensemble algorithms, as introduced by CogACT, are emerging for effective temporal fusion of action predictions. These algorithms dynamically weight past predictions based on their similarity to current predictions, improving the robustness and smoothness of robot movements.
*   **Multi-Stage Data Strategies:** Dealing with data scarcity is being addressed using multi-stage data strategies. GROOT N1 utilizes a data pyramid (web data, human videos, synthetic data, real robot data). It further incorporates techniques like latent action learning and inverse dynamics models to leverage action-less data.
*   **Componentized VLA Architectures:** Componentizing VLA architectures with specialized modules (vision, language, action) is becoming more common. This modularity allows for targeted improvements and scaling of individual components.

### 2. Key Innovations

*   **Enhanced Action Precision:** A key innovation is the ability to generate more precise and temporally coherent robot actions through the integration of diffusion models and adaptive action ensemble techniques.
*   **Generalist Humanoid Robot Foundation Models:** The development of foundation models specifically for generalist humanoid robots, like GROOT N1, marks a significant step towards more versatile and adaptable robots.
*   **Improved Generalization:** Models like CogACT demonstrate superior generalization capabilities to unseen objects, backgrounds, and robots, suggesting advances in learning representations that are less specific to particular environments or robot embodiments.
*   **Data-Efficient Learning from Diverse Sources:** Techniques to leverage data from various sources (web, human videos, synthetic data) with limited real-world robot data are a major innovation. This addresses the long-standing problem of data scarcity in robotics.

### 3. Research Directions

*   **Refining Action Generation with Diffusion Models:** Further research is needed to optimize the architecture and training of diffusion models for action generation in VLAs, including exploring different DiT variants and conditioning strategies.
*   **Improving Inverse Dynamics Modeling:** Improving methods for learning latent action spaces using inverse dynamics models to allow for more effectively using human video demonstrations.
*   **Exploring Hybrid Architectures:** Investigating hybrid architectures that combine the strengths of different action generation methods (e.g., diffusion models with traditional control techniques) could lead to further performance gains.
*   **Scaling and Component Optimization:** Studying the scaling behavior of different VLA components (vision, language, action) to identify bottlenecks and optimize resource allocation is a crucial research direction.
*   **Developing Robust Evaluation Metrics:** Developing more comprehensive and robust evaluation metrics that can capture the full range of capabilities and limitations of VLA models in realistic scenarios remains important.

### 4. Open Challenges

*   **Real-World Deployment Robustness:** Despite advancements in simulated environments, achieving robust and reliable performance in highly unstructured and dynamic real-world environments remains a significant challenge.
*   **Handling Temporal Dependencies:** Accurately capturing and modeling the temporal dependencies in robot actions is crucial for generating smooth and coordinated movements, but remains an open challenge.
*   **Synthesizing Diverse Datasets:** Addressing the long-standing problem of collecting a diverse set of datasets with diverse conditions continues to limit the progress of VLAs.

### 5. Promising Areas for Exploration

*   **Community-Driven Data Collection for Humanoid Robots:** Similar to the Open X-Embodiment efforts, launching initiatives to collect and standardize data from diverse humanoid robot platforms could significantly accelerate progress.
*   **Integration of Reinforcement Learning:** Combining VLAs with reinforcement learning techniques to enable robots to learn from trial and error and adapt to new tasks and environments more effectively.
*   **Developing More Specialized Action Modules:** Exploring different action module architectures that are tailored to specific robot types or task domains could lead to improved performance and efficiency.
*   **Improving Latent Space Action Representations:** Explore different methods for learning latent action spaces that capture the underlying structure and constraints of robot motion.
*   **Investigating Uncertainty-Aware Action Prediction:** Developing methods to quantify and reason about the uncertainty in VLA predictions could improve the safety and reliability of robot control.

---

## ğŸ“„ Papers in This Domain

| Paper | PDF | Date | Authors | GitHub | Citations | Issues | Changes | Twitter |
|-------|-----|------|---------|--------|-----------|--------|---------|----------|
| [OpenVLA: An Open-Source Vision-Langua...](../papers/2406.09246.md) | [2406.09246](https://arxiv.org/abs/2406.09246) | Jun 13, 2024 | Kim et al. | â­[4.4k](https://github.com/openvla/openvla)<br>ğŸ”€[527](https://github.com/openvla/openvla) | [1.1k](https://www.semanticscholar.org/paper/8f9ceb5ffad8e7a066dfc9d9aaa5153b714740ee)<br>ğŸ“ˆ191 | 91 | [Jun 13, 2024](../papers/2406.09246.md) | â¤ï¸[695](https://x.com/moo_jin_kim/status/1801548441102991771) ğŸ”[162](https://x.com/moo_jin_kim/status/1801548441102991771)<br>ğŸ‘ï¸[226.1k](https://x.com/moo_jin_kim/status/1801548441102991771) |
| [Open X-Embodiment: Robotic Learning D...](../papers/2310.08864.md) | [2310.08864](https://arxiv.org/abs/2310.08864) | Oct 13, 2023 | Collaboration et al. | â€” | â€” | None | [Oct 13, 2023](../papers/2310.08864.md) | â¤ï¸[1.2k](https://x.com/GoogleDeepMind/status/1709207886943965648) ğŸ”[302](https://x.com/GoogleDeepMind/status/1709207886943965648)<br>ğŸ‘ï¸[518.8k](https://x.com/GoogleDeepMind/status/1709207886943965648) |
| [RT-2: Vision-Language-Action Models T...](../papers/2307.15818.md) | [2307.15818](https://arxiv.org/abs/2307.15818) | Jul 28, 2023 | Brohan et al. | â­[528](https://github.com/kyegomez/RT-2)<br>ğŸ”€[66](https://github.com/kyegomez/RT-2) | [1.9k](https://www.semanticscholar.org/paper/38939304bb760473141c2aca0305e44fbe04e6e8)<br>ğŸ“ˆ126 | 12 | [Jul 28, 2023](../papers/2307.15818.md) | â¤ï¸[1.6k](https://x.com/GoogleDeepMind/status/1684903412834447360) ğŸ”[435](https://x.com/GoogleDeepMind/status/1684903412834447360)<br>ğŸ‘ï¸[537.1k](https://x.com/GoogleDeepMind/status/1684903412834447360) |
| [GR00T N1: An Open Foundation Model fo...](../papers/2503.14734.md) | [2503.14734](https://arxiv.org/abs/2503.14734) | Mar 18, 2025 | NVIDIA et al. | â€” | [275](https://www.semanticscholar.org/paper/731c50b0d6af4c1cb8d95f506541681ea487973b)<br>ğŸ“ˆ43 | None | [Mar 18, 2025](../papers/2503.14734.md) | â¤ï¸[1.9k](https://x.com/DrJimFan/status/1902117478496616642) ğŸ”[398](https://x.com/DrJimFan/status/1902117478496616642)<br>ğŸ‘ï¸[450.1k](https://x.com/DrJimFan/status/1902117478496616642) |
| [CogACT: A Foundational Vision-Languag...](../papers/2411.19650.md) | [2411.19650](https://arxiv.org/abs/2411.19650) | Nov 29, 2024 | Li et al. | â­[374](https://github.com/microsoft/CogACT)<br>ğŸ”€[33](https://github.com/microsoft/CogACT) | â€” | 7 | [Nov 29, 2024](../papers/2411.19650.md) | â€” |
| [PaLM-E: An Embodied Multimodal Langua...](../papers/2303.03378.md) | [2303.03378](https://arxiv.org/abs/2303.03378) | Mar 06, 2023 | Driess et al. | â­[329](https://github.com/kyegomez/PALM-E)<br>ğŸ”€[46](https://github.com/kyegomez/PALM-E) | [2.1k](https://www.semanticscholar.org/paper/38fe8f324d2162e63a967a9ac6648974fc4c66f3)<br>ğŸ“ˆ99 | 9 | [Mar 06, 2023](../papers/2303.03378.md) | â¤ï¸[694](https://x.com/GoogleAI/status/1634252301303947272) ğŸ”[210](https://x.com/GoogleAI/status/1634252301303947272)<br>ğŸ‘ï¸[173.1k](https://x.com/GoogleAI/status/1634252301303947272) |
| [An Embodied Generalist Agent in 3D World](../papers/2311.12871.md) | [2311.12871](https://arxiv.org/abs/2311.12871) | Nov 18, 2023 | Huang et al. | â­[465](https://github.com/embodied-generalist/embodied-generalist)<br>ğŸ”€[40](https://github.com/embodied-generalist/embodied-generalist) | [252](https://www.semanticscholar.org/paper/13d12b26db345f62e8e512db181b96a7f8763b47)<br>ğŸ“ˆ29 | 0 | [Nov 18, 2023](../papers/2311.12871.md) | â¤ï¸[199](https://x.com/jeasinema/status/1727595460867862930) ğŸ”[34](https://x.com/jeasinema/status/1727595460867862930)<br>ğŸ‘ï¸[81.9k](https://x.com/jeasinema/status/1727595460867862930) |
| [Unleashing Large-Scale Video Generati...](../papers/2312.13139.md) | [2312.13139](https://arxiv.org/abs/2312.13139) | Dec 20, 2023 | Wu et al. | â­[286](https://github.com/bytedance/GR-1)<br>ğŸ”€[15](https://github.com/bytedance/GR-1) | [196](https://www.semanticscholar.org/paper/4443c9a43bff8dcd717e5c75115ec6497af2b953)<br>ğŸ“ˆ17 | 9 | [Dec 20, 2023](../papers/2312.13139.md) | â€” |
| [VIMA: General Robot Manipulation with...](../papers/2210.03094.md) | [2210.03094](https://arxiv.org/abs/2210.03094) | Oct 06, 2022 | Jiang et al. | â­[835](https://github.com/vimalabs/VIMA)<br>ğŸ”€[96](https://github.com/vimalabs/VIMA) | [435](https://www.semanticscholar.org/paper/25425e299101b13ec2872417a14f961f4f8aa18e)<br>ğŸ“ˆ41 | 14 | [Oct 06, 2022](../papers/2210.03094.md) | â¤ï¸[856](https://x.com/DrJimFan/status/1578433493561769984) ğŸ”[147](https://x.com/DrJimFan/status/1578433493561769984) |
| [AHA: A Vision-Language-Model for Dete...](../papers/2410.00371.md) | [2410.00371](https://arxiv.org/abs/2410.00371) | Oct 01, 2024 | Duan et al. | â­[49](https://github.com/NVlabs/AHA)<br>ğŸ”€[3](https://github.com/NVlabs/AHA) | [66](https://www.semanticscholar.org/paper/076f35d58d26ac74c71e6e849dfffc0707aa8a6f)<br>ğŸ“ˆ4 | 3 | [Oct 01, 2024](../papers/2410.00371.md) | â¤ï¸[201](https://x.com/DJiafei/status/1838562171460161619) ğŸ”[43](https://x.com/DJiafei/status/1838562171460161619)<br>ğŸ‘ï¸[48.5k](https://x.com/DJiafei/status/1838562171460161619) |

---

*This page is automatically updated with the latest research trends and papers.*
