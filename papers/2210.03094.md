# VIMA: General Robot Manipulation with Multimodal Prompts

arXiv: [2210.03094](https://arxiv.org/abs/2210.03094) | Published: Oct 06, 2022

**Authors**: Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, Linxi Fan

---

## ğŸ“Š Metrics Summary

| ğŸ“„ PDF | â­ Stars | ğŸ”€ Forks | ğŸ“š Citations | ğŸ“ˆ Influential | â¤ï¸ Likes | ğŸ” Retweets | ğŸ‘ï¸ Views | ğŸ”§ Issues | ğŸ“ PRs | ğŸ¯ Score |
|---------|---------|---------|-------------|---------------|----------|------------|----------|----------|---------|----------|
| [2210.03094](https://arxiv.org/abs/2210.03094) | 835 | 96 | 435 | 41 | 856 | 147 | 0 | 14 | 2 | 27.5 |

**Links**: [GitHub](https://github.com/vimalabs/VIMA) â€¢ [Twitter](https://x.com/DrJimFan/status/1578433493561769984) â€¢ [Semantic Scholar](https://www.semanticscholar.org/paper/25425e299101b13ec2872417a14f961f4f8aa18e)

---

## ğŸ“ Abstract

Prompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. We show that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts, interleaving textual and visual tokens. Accordingly, we develop a new simulation benchmark that consists of thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and a four-level evaluation protocol for systematic generalization. We design a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. VIMA features a recipe that achieves strong model scalability and data efficiency. It outperforms alternative designs in the hardest zero-shot generalization setting by up to $2.9\times$ task success rate given the same training data. With $10\times$ less training data, VIMA still performs $2.7\times$ better than the best competing variant. Code and video demos are available at https://vimalabs.github.io/

---

## ğŸ› Latest Issues

| # | Issue | Created |
|---|-------|----------|
| [#58](https://github.com/vimalabs/VIMA/issues/58) | LVLM Implementation | Sep 12, 2024 |
| [#57](https://github.com/vimalabs/VIMA/issues/57) | How is the current inference frequency? Did someone try it on real world experiment? | Jun 13, 2024 |
| [#56](https://github.com/vimalabs/VIMA/issues/56) | Metrics related to evaluating the models. | Apr 28, 2024 |

[View all issues â†’](https://github.com/vimalabs/VIMA/issues)

---

## ğŸ”„ Recent Activity

| Type | Activity | Author | Date |
|------|----------|--------|------|
| ğŸ”€ PR | [#37](https://github.com/vimalabs/VIMA/pull/37) Update README.md add a specific example to evaluat... | â€” | Sep 26, 2023 |
| ğŸ“ Commit | [`8449837`](https://github.com/vimalabs/VIMA/commit/8449837aa453f8ec9ba229cb956e3bbef5c796ea) Merge pull request #37 from erwincoumans/main | Yunfan Jiang | Sep 26, 2023 |
| ğŸ“ Commit | [`832ea70`](https://github.com/vimalabs/VIMA/commit/832ea70e5482512738d2071fcfe2b73cb3b5cd49) Update README.md | erwincoumans | Sep 25, 2023 |
| ğŸ”€ PR | [#22](https://github.com/vimalabs/VIMA/pull/22) Update README.md | â€” | Jul 31, 2023 |
| ğŸ“ Commit | [`4758671`](https://github.com/vimalabs/VIMA/commit/4758671789ce2b8e94181f917bc3dd948e866fe7) Merge pull request #22 from eltociear/patch-1 | Yunfan Jiang | Jul 31, 2023 |

[View all activity â†’](https://github.com/vimalabs/VIMA/commits)

---

**Last Updated**: 2025-11-14 09:01:19 UTC
