# Diffusion Policy Policy Optimization

arXiv: [2409.00588](https://arxiv.org/abs/2409.00588) | Published: Sep 01, 2024

**Authors**: Allen Z. Ren, Justin Lidard, Lars L. Ankile, Anthony Simeonov, Pulkit Agrawal, Anirudha Majumdar, Benjamin Burchfiel, Hongkai Dai, Max Simchowitz

---

## ğŸ“Š Metrics Summary

| ğŸ“„ PDF | â­ Stars | ğŸ”€ Forks | ğŸ“š Citations | ğŸ“ˆ Influential | â¤ï¸ Likes | ğŸ” Retweets | ğŸ‘ï¸ Views | ğŸ”§ Issues | ğŸ“ PRs | ğŸ¯ Score |
|---------|---------|---------|-------------|---------------|----------|------------|----------|----------|---------|----------|
| [2409.00588](https://arxiv.org/abs/2409.00588) | 681 | 85 | 109 | 16 | 476 | 93 | 76.2k | 23 | â€” | 28.6 |

**Links**: [GitHub](https://github.com/irom-princeton/dppo) â€¢ [Twitter](https://x.com/allenzren/status/1831403337528570132) â€¢ [Semantic Scholar](https://www.semanticscholar.org/paper/e596c98260ec4096eaeb491eb75f91a8339fcf48)

---

## ğŸ“ Abstract

We introduce Diffusion Policy Policy Optimization, DPPO, an algorithmic framework including best practices for fine-tuning diffusion-based policies (e.g. Diffusion Policy) in continuous control and robot learning tasks using the policy gradient (PG) method from reinforcement learning (RL). PG methods are ubiquitous in training RL policies with other policy parameterizations; nevertheless, they had been conjectured to be less efficient for diffusion-based policies. Surprisingly, we show that DPPO achieves the strongest overall performance and efficiency for fine-tuning in common benchmarks compared to other RL methods for diffusion-based policies and also compared to PG fine-tuning of other policy parameterizations. Through experimental investigation, we find that DPPO takes advantage of unique synergies between RL fine-tuning and the diffusion parameterization, leading to structured and on-manifold exploration, stable training, and strong policy robustness. We further demonstrate the strengths of DPPO in a range of realistic settings, including simulated robotic tasks with pixel observations, and via zero-shot deployment of simulation-trained policies on robot hardware in a long-horizon, multi-stage manipulation task. Website with code: diffusion-ppo.github.io

---

## ğŸ› Latest Issues

| # | Issue | Created |
|---|-------|----------|
| [#58](https://github.com/irom-princeton/dppo/issues/58) | Irreproducible result | Oct 15, 2025 |
| [#57](https://github.com/irom-princeton/dppo/issues/57) | A technically basic question | Sep 04, 2025 |
| [#56](https://github.com/irom-princeton/dppo/issues/56) | 0 Improvement after 8M env steps finetuning on Transport | Aug 20, 2025 |

[View all issues â†’](https://github.com/irom-princeton/dppo/issues)

---

## ğŸ”„ Recent Activity

| Type | Activity | Author | Date |
|------|----------|--------|------|
| ğŸ”€ PR | [#31](https://github.com/irom-princeton/dppo/pull/31) Set up proper models in eval | â€” | Feb 04, 2025 |
| ğŸ“ Commit | [`cc7234a`](https://github.com/irom-princeton/dppo/commit/cc7234ad7ff39a8f32de3af903606723a16f0648) add note about `ft_denoising_steps` in eval in REA... | allenzren | Feb 04, 2025 |
| ğŸ“ Commit | [`b8086ed`](https://github.com/irom-princeton/dppo/commit/b8086ed12e00d6f8f1abfe14eb632407340b6ce0) update version | allenzren | Feb 04, 2025 |
| ğŸ“ Commit | [`9032d02`](https://github.com/irom-princeton/dppo/commit/9032d02eaedb7526df7106e9c69f65402a506352) change default `ft_denoising_steps` in eval config... | allenzren | Feb 04, 2025 |
| ğŸ“ Commit | [`fc42865`](https://github.com/irom-princeton/dppo/commit/fc42865c77e8c877dce12301cac6e80e712fc0e4) rename `DiffusionEvalFT` to `DiffusionEval` | allenzren | Feb 04, 2025 |

[View all activity â†’](https://github.com/irom-princeton/dppo/commits)

---

**Last Updated**: 2025-11-14 09:01:43 UTC
