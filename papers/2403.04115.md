# DNAct: Diffusion Guided Multi-Task 3D Policy Learning

arXiv: [2403.04115](https://arxiv.org/abs/2403.04115) | Published: Mar 07, 2024

**Authors**: Ge Yan, Yueh-Hua Wu, Xiaolong Wang

---

## ğŸ“Š Metrics Summary

| ğŸ“„ PDF | â­ Stars | ğŸ”€ Forks | ğŸ“š Citations | ğŸ“ˆ Influential | â¤ï¸ Likes | ğŸ” Retweets | ğŸ‘ï¸ Views | ğŸ”§ Issues | ğŸ“ PRs | ğŸ¯ Score |
|---------|---------|---------|-------------|---------------|----------|------------|----------|----------|---------|----------|
| [2403.04115](https://arxiv.org/abs/2403.04115) | â€” | â€” | â€” | â€” | 88 | 19 | 28.8k | â€” | â€” | 22.8 |

**Links**: [GitHub](#) â€¢ [Twitter](https://x.com/GeYan_21/status/1766323088562786624) â€¢ [Semantic Scholar](#)

---

## ğŸ“ Abstract

This paper presents DNAct, a language-conditioned multi-task policy framework that integrates neural rendering pre-training and diffusion training to enforce multi-modality learning in action sequence spaces. To learn a generalizable multi-task policy with few demonstrations, the pre-training phase of DNAct leverages neural rendering to distill 2D semantic features from foundation models such as Stable Diffusion to a 3D space, which provides a comprehensive semantic understanding regarding the scene. Consequently, it allows various applications to challenging robotic tasks requiring rich 3D semantics and accurate geometry. Furthermore, we introduce a novel approach utilizing diffusion training to learn a vision and language feature that encapsulates the inherent multi-modality in the multi-task demonstrations. By reconstructing the action sequences from different tasks via the diffusion process, the model is capable of distinguishing different modalities and thus improving the robustness and the generalizability of the learned representation. DNAct significantly surpasses SOTA NeRF-based multi-task manipulation approaches with over 30% improvement in success rate. Project website: dnact.github.io.

---

## ğŸ› Latest Issues

No recent issues found.

---

## ğŸ”„ Recent Activity

No recent activity found.

---

**Last Updated**: 2025-11-14 09:01:43 UTC
