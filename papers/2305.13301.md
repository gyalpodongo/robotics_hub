# Training Diffusion Models with Reinforcement Learning

arXiv: [2305.13301](https://arxiv.org/abs/2305.13301) | Published: May 22, 2023

**Authors**: Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, Sergey Levine

---

## ğŸ“Š Metrics Summary

| ğŸ“„ PDF | â­ Stars | ğŸ”€ Forks | ğŸ“š Citations | ğŸ“ˆ Influential | â¤ï¸ Likes | ğŸ” Retweets | ğŸ‘ï¸ Views | ğŸ”§ Issues | ğŸ“ PRs | ğŸ¯ Score |
|---------|---------|---------|-------------|---------------|----------|------------|----------|----------|---------|----------|
| [2305.13301](https://arxiv.org/abs/2305.13301) | 529 | 33 | 558 | 114 | 823 | 177 | 129.4k | 4 | â€” | 32.1 |

**Links**: [GitHub](https://github.com/jannerm/ddpo) â€¢ [Twitter](https://x.com/svlevine/status/1660707076946141184) â€¢ [Semantic Scholar](https://www.semanticscholar.org/paper/d8c78221e4366d6a72a6b3e41e35b706cc45c01d)

---

## ğŸ“ Abstract

Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a vision-language model without the need for additional data collection or human annotation. The project's website can be found at http://rl-diffusion.github.io .

---

## ğŸ› Latest Issues

| # | Issue | Created |
|---|-------|----------|
| [#14](https://github.com/jannerm/ddpo/issues/14) | Rectified Flow Models | Jul 05, 2025 |
| [#13](https://github.com/jannerm/ddpo/issues/13) | About the equation of DDPO_IS in the paper and the code | Nov 09, 2023 |
| [#12](https://github.com/jannerm/ddpo/issues/12) | Reward function on both aesthetic and prompt alignment | Oct 02, 2023 |

[View all issues â†’](https://github.com/jannerm/ddpo/issues)

---

## ğŸ”„ Recent Activity

| Type | Activity | Author | Date |
|------|----------|--------|------|
| ğŸ“ Commit | [`f0b6ca7`](https://github.com/jannerm/ddpo/commit/f0b6ca76516809b9534ad51bd4511117e8eb3682) Update README.md | Kevin Black | Jul 05, 2023 |
| ğŸ“ Commit | [`b217eef`](https://github.com/jannerm/ddpo/commit/b217eef955a94bf58e4de68caa5ec0a6558c221d) Remove some dependencies | Kevin Black | May 30, 2023 |
| ğŸ“ Commit | [`5ea06c3`](https://github.com/jannerm/ddpo/commit/5ea06c30f026ec027a409bc4809776360f893be0) consistency in readme | Michael Janner | May 30, 2023 |
| ğŸ“ Commit | [`734d002`](https://github.com/jannerm/ddpo/commit/734d0029cecf3e911bbcc19793235fedfcc7304b) seq in rwr launch script | Michael Janner | May 30, 2023 |
| ğŸ“ Commit | [`03ead88`](https://github.com/jannerm/ddpo/commit/03ead88964fee8ff88ee9f7aac8caa87c6b02dc6) Deduplicate config | Kevin Black | May 28, 2023 |

[View all activity â†’](https://github.com/jannerm/ddpo/commits)

---

**Last Updated**: 2025-11-14 09:01:42 UTC
