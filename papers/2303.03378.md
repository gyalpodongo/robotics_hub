# PaLM-E: An Embodied Multimodal Language Model

arXiv: [2303.03378](https://arxiv.org/abs/2303.03378) | Published: Mar 06, 2023

**Authors**: Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, Pete Florence

---

## ğŸ“Š Metrics Summary

| ğŸ“„ PDF | â­ Stars | ğŸ”€ Forks | ğŸ“š Citations | ğŸ“ˆ Influential | â¤ï¸ Likes | ğŸ” Retweets | ğŸ‘ï¸ Views | ğŸ”§ Issues | ğŸ“ PRs | ğŸ¯ Score |
|---------|---------|---------|-------------|---------------|----------|------------|----------|----------|---------|----------|
| [2303.03378](https://arxiv.org/abs/2303.03378) | 329 | 46 | 2.1k | 99 | 694 | 210 | 173.1k | 9 | â€” | 45.2 |

**Links**: [GitHub](https://github.com/kyegomez/PALM-E) â€¢ [Twitter](https://x.com/GoogleAI/status/1634252301303947272) â€¢ [Semantic Scholar](https://www.semanticscholar.org/paper/38fe8f324d2162e63a967a9ac6648974fc4c66f3)

---

## ğŸ“ Abstract

Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.

---

## ğŸ› Latest Issues

| # | Issue | Created |
|---|-------|----------|
| [#16](https://github.com/kyegomez/PALM-E/issues/16) | AutoregressiveWrapper typo | Nov 15, 2024 |
| [#15](https://github.com/kyegomez/PALM-E/issues/15) | transfer for caption | Sep 19, 2024 |
| [#14](https://github.com/kyegomez/PALM-E/issues/14) | About SayCan dataset | Jun 04, 2024 |

[View all issues â†’](https://github.com/kyegomez/PALM-E/issues)

---

## ğŸ”„ Recent Activity

| Type | Activity | Author | Date |
|------|----------|--------|------|
| ğŸ“ Commit | [`e27f0a4`](https://github.com/kyegomez/PALM-E/commit/e27f0a49f98cf37af14c3bb0579651e241ee18b5) [README] | Kye | Jan 29, 2024 |
| ğŸ“ Commit | [`77ed6f2`](https://github.com/kyegomez/PALM-E/commit/77ed6f2989e119ae7ef3aae78c705de2410f69de) [VERISON] | Kye | Jan 29, 2024 |
| ğŸ“ Commit | [`70adbc4`](https://github.com/kyegomez/PALM-E/commit/70adbc4efe0ba99b96df17f70e1b0e4c2bbf8bd0) Update model.py | Eternal Reclaimer | Jan 29, 2024 |
| ğŸ“ Commit | [`3b7da94`](https://github.com/kyegomez/PALM-E/commit/3b7da945d30b2bc20f87ed1149398c527f1c8daa) Update pyproject.toml | Eternal Reclaimer | Dec 24, 2023 |
| ğŸ“ Commit | [`2604520`](https://github.com/kyegomez/PALM-E/commit/26045209c6c3d325abe80a9142b8c9c11f44cea4) Update transformer.py | Eternal Reclaimer | Dec 24, 2023 |

[View all activity â†’](https://github.com/kyegomez/PALM-E/commits)

---

**Last Updated**: 2025-11-14 09:01:26 UTC
